<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="chap1-chapter" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Basic Concepts of Quantum Computation</title>
  
  <xi:include href="chap1-intro.ptx" />
 

  <section xml:id="Classical-Physics">
    <title> Classical Physics </title>
    <p> Classical physics, a cornerstone of scientific understanding, encompasses several branches that describe the physical phenomena of the universe, laying the groundwork for modern science.  Optics, electromagnetism, nuclear physics, astrophysics, and thermodynamics are pivotal fields that have evolved through centuries of inquiry and discovery. Optics, the study of light and its interactions with matter, dates back to ancient civilizations. The Greeks, particularly Euclid and Ptolemy, made early contributions with theories on reflection and refraction.  </p>
    <p> In the 17th century, Johannes Kepler elucidated the principles of lenses and vision, while Galileo's telescopic observations expanded our understanding of the cosmos. Isaac Newton's corpuscular theory of light and Christian Huygens' wave theory were significant milestones, reconciled in the 19th century by experiments demonstrating the wave nature of light. </p>
    <p> Electromagnetism, dealing with electric and magnetic fields and their interactions, began in the 18th century with static electricity and magnetism as separate phenomena. Benjamin Franklin's experiments with lightning and Luigi Galvani's work on bioelectricity were early milestones. The unification of electricity and magnetism into a single theory was achieved by James Clerk Maxwell in the mid-19th century, whose equations predicted the existence of electromagnetic waves, leading to Heinrich Hertz's discovery of radio waves. </p>
    <p> Nuclear physics, focusing on atomic nuclei, emerged in the early 20th century, building on classical foundations. The discovery of radioactivity by Henri Becquerel and the subsequent work of Marie and Pierre Curie opened new frontiers. Ernest Rutherford's gold foil experiment, which revealed the nucleus of an atom, was pivotal. Early developments in nuclear decay laws and the identification of radiation types owe much to classical methodologies. </p>
    <p> Astrophysics, applying physics to celestial objects and phenomena, has ancient origins in observational astronomy. Early contributions by Hipparchus and Ptolemy laid the groundwork, but the Copernican Revolution in the 16th century and Johannes Kepler's laws of planetary motion advanced the field. Newton's law of universal gravitation unified terrestrial and celestial mechanics under a single theory, revealing that the force that moved the stars was the same one that held us to the ground: gravity. </p>
    <p> Thermodynamics, the study of heat, work, and energy, emerged from the Industrial Revolution's practical needs. Sadi Carnot's work on steam engines laid the foundation for the field. The first and second laws of thermodynamics, formulated by Rudolf Clausius and William Thomson (Lord Kelvin), established principles governing energy conservation and entropy, profoundly influencing scientific thought and engineering practice. </p>
    <p> Classical physics is crucial academically as it forms the foundational bedrock upon which modern physics and other scientific disciplines are built. It introduces students to fundamental concepts such as Newtonian mechanics, thermodynamics, and electromagnetism, cultivating critical thinking and problem-solving skills. Mastery of classical physics fosters an appreciation for the scientific method and the historical context of discoveries, illustrating the interplay of theoretical advances and experimental validations. Serving as an indispensable educational foundation, classical physics equips students with the knowledge and skills necessary for careers in science, engineering, and technology, continuing to inspire future innovations and discoveries. </p>
</section>

<section xml:id="Newtonian-Mechanics">
  <title> Newtonian Mechanics</title>
  <p> Many of the concepts behind Classical Physics are motivated by Newtonian Mechanics, which is derived from Newton’s three laws of motion. These laws provide a framework for determining how an object will move when acted upon (or not acted upon) by external forces. Newton’s three laws were first described in his <i>Philosophiæ Naturalis Principia Mathematica</i>, which is widely considered to be one of the most important works in the history of physics. <!-- \cite{newton1850newton} -->  </p>
  <p> Newton’s first law, put simply, is that in the absence of external forces, an object at rest will stay at rest and an object in motion will stay in motion. Furthermore, without outside interference, an object in motion will travel at a constant speed in a straight line. In <i>Principia</i>, Newton defines inertia as the property of matter to preserve its current state and resist attempts to change it. <!-- \cite{newton1850newton} --> Thus, the first law of motion establishes that all objects have the property of inertia and resist changes to their state of rest or motion. </p>
  <p> The second law of motion is often abbreviated by the equation <m> \sum \vec{F} = m\vec{a}. </m> This equation tells us that the acceleration (<m>\vec{a}</m>) of an object is directly proportional to the net force (<m>\sum \vec{F}</m>) acting upon it and is inversely proportional to the mass (<m>m</m>) of the object. Mass can be thought of as a numerical measure of inertia, so the second law relates to the first law in that objects with a lot of mass, i.e. objects with a lot of inertia, require greater forces to accelerate. Furthermore, if we look at the case where there are no forces acting upon an object, i.e. <m>\sum \vec{F}=0</m>, we know that <m>m\vec{a}=0</m> as well. Since the mass of an object can never be <m>0</m>, when there are no forces acting upon an object, it must have an acceleration of <m>0</m>. In the absence of acceleration, an object at rest will stay at rest and an object in motion will continue to travel at a constant speed. </p>
  <p> Newton’s third law states that when two objects interact, the force exerted by the first object on the second is equal in magnitude and opposite in direction to the force exerted by the second object on the first. Simply put, every action has an equal and opposite reaction.  </p>
  <p> The three laws of motion can be seen in the example of a rocket launching into space. As the thrusters begin to fire, the rocket does not immediately begin to move. Since the rocket begins at rest, its inertial properties resist changing to a state of motion, which showcases the first law. Once the rocket begins to lift off the ground, it appears to be moving upwards slowly. Since the rocket is very heavy, it has a lot of mass, and since acceleration is inversely proportional to mass by Newton’s second law, it thus accelerates slowly. The third law of motion can be seen in the fact that the rocket engines are pushing down, which causes the equal and opposite reaction of the rocket to move upwards.  </p>
  <activity>
    <video xml:id="yt-list-vars" youtube="fhYMh6KTJMQ" width="65%" margins="35%" />
  </activity>
</section>

<section xml:id="Classical-Computing">
  <title> Classical Computing </title>

  <p> Before diving into the basics of quantum computing, we must first discuss how computing is done classically. On non-quantum computers, information is stored in strings of bits, where each bit is either a 0 or a 1. These strings of bits represent numbers in binary and they provide computers with instructions on what to do. The notable difference on a quantum computer is that quantum bits (qubits) exist in a <term>superposition</term><idx><h>Superposition</h></idx> of states that allows them to be both a 0 and a 1 until they are measured and this superposition collapses. This unique property of qubits allows quantum computers to perform multiple processes simultaneously. </p>
  <p>When describing the state of a qubit, instead of writing 0 and 1, we use <m>\ket{0}</m> and <m>\ket{1}</m>. These quantum states belong to a <term>vector space</term><idx><h>Vector</h><h>Vector Space</h></idx>, which means that multiplying states by a constant coefficient and adding states together will result in another valid quantum state. This is how a superposition is formed, by creating a linear combination of the states <m>\ket{0}</m> and <m>\ket{1}</m>.</p> 
</section>

  <section xml:id="Vectors-and-Vector-Spaces">
    <title> Vectors and Vector Spaces</title>
    <subsection xml:id="Vectors">
      <title>Introduction to Vectors</title>
    <p>
      A vector is an ordered list of numbers that is used to describe quantities with both magnitude and direction. One example of a vector is force, which has both a magnitude (how strong the force is) and a direction (the angle at which the force is being applied). Each vector has a dimension, which is the number of components that comprise it. It is customary to signify that something is a vector by either drawing an arrow on top of it or bolding it. If we have a vector called <m>x</m>, we would write <m>\vec{x}</m> or <m>\boldsymbol{x}</m>. The number <m>x_{n}</m> is called the <m>n</m>-th component of <m>\vec{x}</m>. </p> <p>The most typical use of a vector with <m>n</m> components is to describe a point in <m>n</m> dimensional space in reference to some starting point. For example, if you have a square piece of paper and label the bottom left corner with the starting point (0,0), then a vector with components (1,1) would represent moving one unit of measurement along the bottom of the paper and one unit of measurement along the side of the paper to reach a new point. This vector would have a direction of <m>45^{\circ}</m> and a magnitude of <m>\sqrt{2}</m> units (by the pythagorean theorem). 
      </p>
      <image source="VectorExample.png">
        <shortdescription>An example of a vector</shortdescription>
      </image>
    </subsection>
    <subsection xml:id="Vector-Spaces">
      <title>Vector Spaces</title>
  <p> Every vector exists within a <term>vector space</term><idx><h>Vector</h><h>Vector Space</h></idx>, which are sets that satisfy certain mathematical properties. The most common vector spaces we will deal with are <m>\mathbb{R}^n</m>, the set of all <m>n</m> dimensional vectors with real components, and <m>\mathbb{C}^n</m>, the set of all <m>n</m> dimensional vectors with complex components. Notice <m>\mathbb{R}^n \subseteq \mathbb{C}^n</m>, so we will usually work with <m>\mathbb{C}^n</m> for generality. All vector spaces have the same properties regardless of dimension. Here are the properties for an <m>n</m>-dimensional vector space:   
    </p>
    <p>1. Vector equality: <m>\vec{x}=\vec{y}</m> means <m>x_{i}=y_{i}, i \in n. </m> </p>
      <p>2. Vector addition: <m>\vec{x}+\vec{y}=\vec{z}</m> means <m>x_{i}+y_{i}=z_{i}, i \in n. </m> </p>
        <p>3. Scalar multiplication: <m>a \vec{x} \equiv \big(a x_{1}, a x_{2},..., a x_{n}\big). </m> </p>
          <p>4. Negative of a vector: <m>-\vec{x}=(-1) \vec{x} \equiv \big(-x_{1},-x_{2},...,-x_{n}\big).  </m> </p>
            <p>5. Null vector: There exists a null vector <m>\vec{0} \equiv (0,0,...,0).  </m> </p>
<p> If our vector components are all real numbers (i.e. the vector space exists in <m>\mathbb{R}^n</m> instead of <m>\mathbb{C}^n</m>), then the following properties also hold: </p>
<p>1. Addition of vectors is commutative: <m>\vec{x}+\vec{y}=\vec{y}+\vec{x}. </m></p>
<p>2. Addition of vectors is associative: <m>(\vec{x}+\vec{y})+\vec{z}=\vec{x}+(\vec{y}+\vec{z}). </m></p>
<p>3. Scalar multiplication is distributive: <m>a(\vec{x}+\vec{y})=a \vec{x}+a \vec{y},</m> and <m>(a+b) \vec{x}=a \vec{x}+b \vec{x}</m></p>
<p>4. Scalar multiplication is associative: <m>(a b) \vec{x}=a(b \vec{x}).</m> </p>
</subsection>
<subsection xml:id="Bases">
  <title>Bases</title>
  <p> For any vector space, one can find a subset of vectors which can be used to generate any other vector in the space through linear combinations (scalar multiplication and vector addition). The smallest set of vectors that fulfills this property is called the <term>basis</term><idx><h>Vector</h><h>basis</h></idx>. In <m>\mathbb{R}^{2}</m>, we only need two vectors to produce the rest through linear combination. The standard basis, <m>\mathcal{B},</m> is:  </p>
  <me>
    {\mathcal{B}}=\bigg\{ \{\widehat{\mathbf{x}}, \widehat{\mathbf{y}}\}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \bigg\}
  </me>
  <p> The ^ symbol is used to denote that a vector is <term>normal</term><idx><h>Vector</h><h>Normal</h></idx>, which means that it has a length of 1. The vector <m>\widehat{x}</m> is referred to as "<m>x</m> hat." This property is extremely important to quantum mechanics and will be discussed more later. </p>

<p> Bases have two properties: </p>
<p> <alert>Linear Independence: </alert> A set of vectors is <term>linearly independent</term><idx><h>Vector</h><h>Linearly Independent</h></idx> if we cannot express any one of them as a linear combination of the others. If we can express one as a linear combination of the others, then it is called a <term>linearly dependent</term><idx><h>Vector</h><h>Linearly Dependent</h></idx> set. A basis must be linearly independent. </p>
<p> <alert>Completeness: </alert> A set of vectors is complete if it <term>spans</term><idx><h>Vector</h><h>Span</h></idx> its vector space, which in turn means that any vector in the space can be expressed as a linear combination of the vectors in that set. A basis for a vector space must span the space. </p>
</subsection>
<example>
  <title>(A Counterexample)</title>
<p>Let <m>{\mathcal{B}}</m> be the set,</p>
<me> {\mathcal{B}} = \Bigg\{ \begin{pmatrix} 1\\ 0\\ 0\\ \end{pmatrix}, \begin{pmatrix} 0\\ 1\\ 0\\ \end{pmatrix} \Bigg\} </me>
  <p> And let <m>\vec{v}</m> be the vector, </p>
  <me> \vec{v}= \begin{pmatrix} 2\\ -3\\ 1\\ \end{pmatrix} </me>
  <p> Since we are unable to express <m>\vec{v}</m> as a linear combination of the elements of <m>\mathcal{B}</m>, then we say <m>{\mathcal{B}}</m> is not complete. </p>
</example>
<example>
  <p>Let <m>{\mathcal{B}}</m> be the set,</p>
  <me> {\mathcal{B}} = \Bigg\{ \begin{pmatrix} 1\\ 0\\  \end{pmatrix}, \begin{pmatrix} 0\\ 1\\ \end{pmatrix} \Bigg\} </me>
  <p> And let <m>\vec{x}</m> be the vector, </p>
  <me> \vec{x} = \begin{pmatrix} 5\\ -7\\ \end{pmatrix} </me>
  We can express <m>\vec{x}</m> as:
  <me> \vec{x} = 5 \cdot \begin{pmatrix} 1\\ 0\\ \end{pmatrix} + (-7) \cdot \begin{pmatrix} 0\\ 1\\ \end{pmatrix} = \begin{pmatrix}5\\0\\ \end{pmatrix} + \begin{pmatrix}0\\ -7\\ \end{pmatrix} = \begin{pmatrix} 5\\ -7\\ \end{pmatrix} </me>
  <p> Since we can express <m>\vec{x}</m> as a linear combination of the elements of <m>\mathcal{B}</m> and it is easy to show that we could construct any other vector in <m>\mathbb{R}^2</m> from these same elements, we say that <m>\mathcal{B}</m> spans <m>\mathbb{R}^2</m> and is complete. </p>
</example>
<theorem xml:id="thm-1"><statement><p><alert> Dimension of a basis. </alert> The number of basis elements for a vector space is the same as that spaces dimension.  </p></statement></theorem>

<subsection xml:id="linear-algebra">
  <title>Linear Algebra</title>
  <p>Linear algebra is the study of vectors and transformations. In this subsection we will describe some other pieces of linear algebra that will be important to quantum computation.</p>
 <p><alert>Vector Transpose:</alert> The <term>transpose</term><idx><h>Vector</h><h>Transpose</h></idx> is an operation that turns a standard column vector into a row vector, or vice versa. This means an <m>n</m> dimensional vector changes from having <m>n</m> rows and <m>1</m> column to having <m>1</m> row and <m>n</m> columns. The transpose is represented with a superscript <m>T</m> and the operation is shown below. </p>
<me> \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}^{T} = \begin{pmatrix} a_1 &amp; a_2 &amp; \ldots &amp; a_n \end{pmatrix} </me>
<p><alert>Dot Product / Inner Product:</alert> The dot product (more generally known as the <term>inner product</term><idx><h>Vector</h><h>Inner Product</h></idx> in the context of quantum computation) is an operation between two vectors of the same dimension that produces a scalar. This product is typically referred to with a <m>\cdot</m>, but has an alternate notation in quantum computation that we will see in the next section. In <m>\mathbb{R}^n</m> and In <m>\mathbb{C}^n</m> this operation is performed by taking the sum of the products of the corresponding entries in each vector, as shown below.  </p>
<me> \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix} \cdot \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{pmatrix} = a_1 \cdot b_1 + a_2 \cdot b_2 + \ldots + a_n \cdot b_n </me>
<example>
  <me> \begin{pmatrix} 5 \\ 3 \\ 7\\ 2 \end{pmatrix} \cdot \begin{pmatrix} 6 \\ 0 \\ 5 \\ 1 \end{pmatrix} = 5 \cdot 6 + 3 \cdot 0 + 7 \cdot 5 + 2 \cdot 1 = 67 </me>
</example>
<exercise>
  <statement><p> Find the inner product <m>\vec{v_1} \cdot \vec{v_2} </m> </p>
    <me>\vec{v_1} = \begin{pmatrix} 8 \\ -4 \\ 5 \end{pmatrix}, \text{  } \vec{v_2} = \begin{pmatrix} 3 \\ 5 \\ -2 \end{pmatrix} </me> </statement> 
    <solution><me> \begin{pmatrix} 8 \\ -4 \\ 5 \end{pmatrix} \cdot \begin{pmatrix} 3 \\ 5 \\ -2 \end{pmatrix} = 8 \cdot 3 + -4 \cdot 5 + 5\cdot -2 = -6 </me></solution>
</exercise>
<p><alert>Orthogonality:</alert> <term>Orthogonality</term><idx><h>Orthogonality</h></idx> is the generalization of the concept of perpendicularity. In two and three dimensional space, two vectors are orthogonal if the angle between them is a right angle. Two vectors are orthogonal if their inner product is equal to <m>0</m>.</p>
<exercise>
  <statement><p> Are the following vectors orthogonal? </p>
  <me>\begin{pmatrix} 4 \\ 3 \\ -2 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 5 \end{pmatrix}</me></statement>
  <solution><p>Yes,</p><me> \begin{pmatrix} 4 \\ 3 \\ -2 \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 2 \\ 5 \end{pmatrix} = 4 \cdot 1 + 3 \cdot 2 + -2 \cdot 5 = 0</me><p>Since the inner product between the two vectors is 0, they are orthogonal</p></solution> 
</exercise>
<p><alert>Normality:</alert> A vector is <term>normal</term><idx><h>Vector</h><h>Normal Vector</h></idx> if it has a length of <m>1</m>. The length (sometimes also referred to as the norm) of a vector can be found be taking the square root of the sum of the squares of its entries, as shown below. This operation is represented by lines(<m>\| \ \|</m>) on either side of the vector that is having its length found. A non-normal vector can be normalized by dividing each of its components by the vectors length. A normalized vector has the same direction as the original vector, but has a length of <m>1</m>. A set of vectors is <term>orthonormal</term><idx><h>Vector</h><h>Orthonormal Vectors</h></idx> if each of the vectors are normal and each of the vectors are orthogonal to the rest. </p>
<me> \| (\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}) \| = \sqrt{a_{1}^2 + a_{2}^2 + \ldots + a_{n}^2} </me>
<exercise>
  <statement><p>What is the length of the following vector?</p><me>\begin{pmatrix} 7 \\ 2 \\ 3 \end{pmatrix}</me></statement> 
  <solution> <me> \sqrt{7^2 + 2^2 + 3^2} = \sqrt{62} \approx 7.874 </me> </solution> 
</exercise>  
<exercise>
  <statement><p>Normalize the following vector</p><me>\begin{pmatrix} 7 \\ 2 \\ 3 \end{pmatrix}</me></statement>
  <solution><p>In the previous exercise, we found that this vector has a length of <m>\sqrt{62}\approx7.874</m>. To normalize the vector, we will divide each of its components by its length.</p>
    <me> \begin{pmatrix} \frac{7}{\sqrt{62}} \\ \frac{2}{\sqrt{62}} \\ \frac{3}{\sqrt{62}} \end{pmatrix} \approx \begin{pmatrix} 0.889 \\ 0.254 \\ 0.381 \end{pmatrix}</me></solution>
</exercise>
<p><alert>Matrices:</alert> Whereas a vector is a single column of elements, a <term>matrix</term><idx><h>Matrix</h></idx> is a table of elements organized in rows and columns. Technically speaking, a vector can be thought of a matrix with only one column. The dimension of a matrix is described by first listing the number of rows and then listing the number of columns. Thus, a <m>2 \times 3</m> matrix (read "two by three") would have two rows and three columns. A <term>square matrix</term><idx><h>Matrix</h><h>Square Matrix</h></idx> is any matrix with the same number of rows and columns. One of the most important matrices is the <term>identity matrix</term><idx><h>Matrix</h><h>Identity Matrix</h></idx>, a square matrix in which all of the entries along the diagonal are <m>1</m> and all other entries are <m>0</m>. Examples of the <m>2\times2</m> and <m>5\times5</m> identity matrices are shown below, which can be generalized to any <m>n \times n</m> matrix.</p>
  <me> I_{2\times2} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, I_{5\times5} = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1   \end{pmatrix}</me>
  <p> A matrix can be multiplied by a scalar in the same way that a vector can, by multiplying each entry in the matrix by the scalar, as shown below. </p>
    <me> c \times \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} = \begin{pmatrix} c \times a_1 &amp; c \times a_2 &amp; c \times a_3 \\ c \times a_4 &amp; c \times a_5 &amp; c \times a_6 \\ c \times a_7 &amp; c \times a_8 &amp; c \times a_9 \end{pmatrix} </me>
 <p> The concept of a transpose can be extended from vectors to matrices as well. A matrix's transpose is found by turning each of its rows into a column, or, equivalently, by turning each of its columns into a row. This means the transpose of an <m>m \times n</m> matrix is an <m>n \times m</m> matrix. The matrix transpose is also represented by a superscript <m>T</m>. An example of transposing a <m>3 \times 3</m> matrix is shown below. </p>
 <me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} ^{T} = \begin{pmatrix} a_1 &amp; a_4 &amp; a_7 \\ a_2 &amp; a_5 &amp; a_8 \\ a_3 &amp; a_6 &amp; a_9 \end{pmatrix} </me>
    <p><alert>Matrix Addition:</alert> Two matrices can be added together only if they each have the same number of rows and columns. The sum of two matrices is found by adding together the corresponding entries of each matrix, as shown below with an example of two <m>3\times3</m> matrices. </p>
  <me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} + \begin{pmatrix} b_1 &amp; b_2 &amp; b_3 \\ b_4 &amp; b_5 &amp; b_6 \\ b_7 &amp; b_8 &amp; b_9 \end{pmatrix} = \begin{pmatrix} a_1 + b_1 &amp; a_2 + b_2 &amp; a_3 + b_3 \\ a_4 + b_4 &amp; a_5 + b_5 &amp; a_6 + b_6 \\ a_7 + b_7 &amp; a_8 + b_8 &amp; a_9 + b_9 \end{pmatrix} </me>
  <exercise> 
  <statement>
    <p> Compute the following sum: </p>
    <me> \begin{pmatrix} 4 &amp; 7 &amp; 9 \\ 1 &amp; 0 &amp; 6 \\ 2 &amp; 5 &amp; 3 \end{pmatrix} + \begin{pmatrix} 0 &amp; 4 &amp; 6 \\ 2 &amp; 2 &amp; 2 \\ 5 &amp; 3 &amp; 8 \end{pmatrix}</me>
  </statement>  
  <solution>
    <me> = \begin{pmatrix} 4 &amp; 11 &amp; 15 \\ 3 &amp; 2 &amp; 8 \\ 7 &amp; 8 &amp; 11 \end{pmatrix} </me>
  </solution>
  </exercise>
<p><alert>Matrix Multiplication:</alert> Two matrices can be multiplied together only if the number of columns of the left matrix is equal to the number of rows of the right matrix. The resulting product will be a matrix with the same number of rows as the left matrix and the same number of columns as the right. Thus an <m>m /times n</m> matrix multiplied by a <m> n \times q</m> matrix would produce an <m>m \times q</m> matrix. Notably, matrix multiplication is non-commutative, which means for two matrices <m>A</m> and <m>B</m>, <m>AB \neq BA</m>. The entries of a product matrix are determined by taking the dot product between the corresponding row of the left matrix and the corresponding column of the right matrix, as shown below with an example of multiplication between a <m>3\times3</m> and a <m>3\times2</m> matrix. </p>
<me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} \times \begin{pmatrix} b_1 &amp; b_2 \\ b_3 &amp; b_4 \\ b_5 &amp; b_6 \end{pmatrix} = \begin{pmatrix} a_1b_1 + a_2b_3+a_3b_5 &amp; a_1b_2 + a_2b_4 + a_3b_6 \\ a_4b_1 + a_5b_3 + a_6b_5 &amp; a_4b_2 + a_5b_4 + a_6b_6 \\ a_7b_1 + a_8b_3 + a_9b_5 &amp; a_7b_2 + a_8b_4 + a_9b_6 \end{pmatrix} </me>
<p> Now that we know matrix multiplication, we can redefine the dot product / inner product as the transpose of a vector multiplied by the original vector. Since vectors are <m>n \times 1</m> dimensional, the transpose will be <m>1 \times n</m> dimensional, so multiplying the transpose on the left and the original on the right will produce a <m>1 \times 1</m> matrix, which is functionally the same as a scalar. </p>
<p> Multiplying an <m> m \times n</m> matrix by a <m>n \times 1</m> vector produces an <m>m \times 1</m> vector. This operation is known as a <term>linear transformation</term><idx><h>Linear Transformation</h></idx> and it an be used to move vectors from one vector space to another. </p>
<exercise>
  <statement> <p> Compute the following product: </p> <me> \begin{pmatrix} 2 &amp; 4 \\ 3 &amp; 0 \end{pmatrix} \times \begin{pmatrix} 5 &amp; 1 &amp; 2 \\ 0 &amp; 7 &amp; 2 \end{pmatrix} </me> </statement>
    <hint> <p> The resulting matrix should be <m>2 \times 3</m> </p></hint>
    <solution> <me> = \begin{pmatrix} 2 \times 5 + 4 \times 0 &amp; 2 \times 1 + 4 \times 7 &amp; 2 \times 2 + 4 \times 2 \\ 3 \times 5 + 0 \times 0 &amp; 3 \times 1 + 0 \times 7 &amp; 3 \times 2 + 0 \times 2 \end{pmatrix} = \begin{pmatrix} 10 &amp; 30 &amp; 12 \\ 15 &amp; 3 &amp; 6 \end{pmatrix} </me> </solution> </exercise>
<p><alert>Determinant:</alert> The <term>determinant</term><idx><h>Determinant</h></idx> is a scalar associated with a square matrix that can be used to find the eigenvalues (see <xref ref="subsec-Eigenvectors-and-Eigenvalues"/>) of the matrix. For <m>2 \times 2</m> and <m>3 \times 3</m> matrices, their determinants are defined as follows: </p>
<me> det(\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}) = a \cdot d - b \cdot c </me>
<me> det(\begin{pmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{pmatrix}) = a \cdot e \cdot i +  b \cdot f \cdot g + c \cdot d \cdot h - c \cdot e \cdot g - b \cdot d \cdot i - a \cdot f \cdot h </me>
  </subsection>
<subsection xml:id="subsec-Eigenvectors-and-Eigenvalues">
  <title>Eigenvectors and Eigenvalues</title>
  <p> Here we will provide a brief overview of eigenvectors and eigenvalues, but readers wishing to learn more should go to this link <url href="https://tinyurl.com/mhtupbk6" />. An eigenvector of a matrix <m>A</m> is a non-zero vector <m>\vec{v}</m> such that </p> 
<me> A \vec{v} = v\vec{v} </me> 
<p> where <m>v</m> is a scalar known as the eigenvalue of <m>A</m> corresponding to <m>\vec{v}</m>. It will often be convenient to use the variable <m>v</m> both as a label for the eigenvector, and to represent the eigenvalue. The eigenvalues of a matrix can be found by solving the equation </p> 
<me> \text{det}(A-\lambda I) = 0 </me>
<p> In this equation, <m>\lambda</m> is an unknown variable whose values we want to find. These values will be the eigenvalues of the matrix <m>A</m>. Each eigenvalue <m>\lambda_i</m> will have a corresponding eigenvector <m>\vec{v_i}</m> such that <m>A\vec{v_i} = \lambda_i \vec{v_i}</m>. The corresponding eigenvectors can then be found by solving</p> 
<me> (A-\lambda_i I)\vec{v_i} = \vec{0} </me>
<p>The eigenspace corresponding to an eigenvalue <m>v</m> is the set of vectors which have eigenvalue <m>v</m>. It is a vector subspace of the vector space on which <m>A</m> acts. The eigenspace corresponding to the matrix <m>A</m> would be all vectors that have an eigenvalue for that matrix. </p>
</subsection>
</section>

<section xml:id="Linear-Algebra-in-Quantum-Computation">
  <title>Linear Algebra in Quantum Computation</title>
 <p> As mentioned in the section on Classical Computing <xref ref="Classical-Computing"/>, qubits can be in the state <m>\ket{0}</m> or <m>\ket{1}</m>. What's more, these states can be represented with vectors. </p>
  <me> \ket{0} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} </me>
  <p> and </p>
  <me> \ket{1} = \begin{pmatrix} 0 \\ 1 \end{pmatrix} </me>
  <p> This notation was first introduced by mathematician Paul Dirac and is known as "Dirac Notation" or "<term>Bra-Ket Notation</term><idx><h>Bra-Ket Notation</h></idx>." In this notation, the <m>\ket{}</m> symbol represents a qubit state and is referred to as a "<term>ket</term><idx><h>Bra-Ket Notation</h><h>Ket</h></idx>." At a glance, it can be seen that the vectors <m>\ket{0}</m> and <m>\ket{1}</m> are each normal and are orthogonal to each other. Additionally, any point in two dimensional space could be described with a linear combination of these two vectors, meaning they form a basis (we will discuss exactly which space they form a basis for in the following section). Put together, this means <m>\ket{0}</m> and <m>\ket{1}</m> form an orthonormal basis. All qubits must be normalized in order to be expressed properly. </p>
  <p> Systems with multiple qubits are described by vectors in higher dimensions, which will be discussed later. </p>
</section>

<section xml:id="Qubit-States">
  <title>Qubit States</title>
  <subsection xml:id="subsec-Qubit">
    <title>Qubits</title>
 <p> As a reminder, a qubit exists in a superposition between the states <m>\ket{0}</m> and <m>\ket{1}</m>, which means that they can be expressed as a linear combination of the vectors <m>\begin{pmatrix} 1 \\ 0 \end{pmatrix}</m> and <m> \begin{pmatrix} 0 \\ 1 \end{pmatrix} </m>. This means a qubit <m> \psi </m> can be expressed </p>
 <me> \ket{\psi} = \alpha \ket{0} + \beta \ket{1} </me>
 <p> Where <m> \alpha </m> and <m> \beta </m> are complex coefficiants that relate to the probability that a qubit is in the states <m> \ket{0}</m> and <m> \ket{1} </m> respectively. This means that qubits exist within a complex vector space. The exact space that qubits exist within is known as a <term>hilbert Space</term><idx><h>Hilbert Space</h></idx>. </p>
 </subsection>
<subsection xml:id="subsec-Hilbert-Space">
  <title>Hilbert Space</title>
  <p> A hilbert space is a complex vector space in which the inner product is defined as an operation. A single qubit exists within the hilbert space <m>\mathbb{C}^2</m>, which is the set of all two dimensional vectors with complex entries. </p>
 </subsection>
 <subsection xml:id="subsec-Bra-Vectors-and-the-Inner-Product">
  <title>Bra Vectors and the Inner Product for Quantum Computation</title>
  <p> We now introduce the bra in Bra-Ket Notation. Each vector representation of a qubit has a corresponding <term>bra</term><idx><h>Bra-Ket Notation</h><h>Bra</h></idx> vector (sometimes also referred to as the "Dual Vector") that is equal to the complex conjugate of the transpose of the ket. This operation of taking the transpose and complex conjugating is known as the <term>adjoint</term><idx><h>Adjoint</h></idx> and is represented by the <m>\dagger</m> symbol. Bra vectors are represented by the symbol <m>\bra{}</m>. An example of finding the bra vector for the ket <m>\psi</m> is shown below (where the <m>*</m> symbol represents complex conjugating).</p>
<me> \ket{\psi}^{\dagger} = (\alpha \ket{0} + \beta \ket{1})^{\dagger} = (\alpha \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \beta \begin{pmatrix} 0 \\ 1 \end{pmatrix})^{\dagger} = \alpha^{*} \begin{pmatrix} 1 &amp; 0 \end{pmatrix} + \beta^{*} \begin{pmatrix} 0 &amp; 1 \end{pmatrix} = \bra{\psi} </me>
<p> Bra vectors allow us to use a new notation for the inner product. We write the inner product between two vectors, <m>\ket{\psi}</m> and <m>\ket{\phi}</m>, as <m>\braket{\psi | \phi}</m> and perform the calculation as matrix multiplication, as described in <xref ref="linear-algebra"/> </p>
<exercise>
  <statement><p>Find the inner product between the following two qubits.</p>
  <me> \ket{\psi} = \frac{1}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1} = \begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix} </me>
  <me> \ket{\phi} = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt2} \ket{1} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} </me></statement>
<solution>
<me> \ket{\psi} \cdot \ket{\phi} = \braket{\psi|\phi} = \begin{pmatrix} \frac{1}{2} &amp; \frac{\sqrt{3}}{2} \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} = \frac{1}{2} \cdot \frac{1}{\sqrt{2}} + \frac{\sqrt{3}}{2} \cdot \frac{1}{\sqrt{2}} = \frac{1 + \sqrt3}{2\sqrt{2}}</me></solution>
</exercise>
</subsection>
 <subsection xml:id="subsec-Qubit-Measurements">
  <title>Qubit Measurements</title>
  <p> When a qubit is measured, it collapses from being a superposition of the states <m>\ket{0}</m> and <m>\ket{1}</m> to being in one state or the other. The probability that the qubit will be in either state is related to the coefficient on that state. Consider a qubit <m> \psi </m>, </p>
<me> \ket{\psi} = \alpha \ket{0} + \beta \ket{1} = \alpha \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \beta \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} \alpha \\ \beta \end{pmatrix} </me> 
<p>Since all qubits are normalized, <m>\psi</m> has a length of <m>1</m>, so <m>\sqrt{\alpha^2 + \beta^2}=1</m>, which means <m>\alpha^2 + \beta^2=1</m>.  Since the combined probability that the qubit will be in the state <m>\ket{0}</m> or <m>\ket{1}</m> is equal to <m>1</m>, we use the normal property of qubits as a method to determine the probability that the qubit will be in either state. For the qubit <m>\psi</m> the probability that it will be in the state <m>\ket{0}</m> after it is measured is <m>\alpha^2</m> and the probability that it will be in the state <m>\ket{1}</m> is <m>\beta^2</m>. </p>
<p><alert>Born Rule:</alert> The Born Rule tells us that if we express a qubit as a linear combination of basis states, then upon measurement, the probability that the qubit collapses into any given basis state is equal to the square of the coefficient for that basis state. A qubit <m>\ket{\psi}</m> in a vector space defined by the basis <m> \{ \ket{\beta_1},\ket{\beta_2},\ldots,\ket{\beta_n} \} </m> </p>
<me> \ket{\psi} = \alpha_1 \ket{\beta_1} + \alpha_2 \ket{\beta_2} + \ldots + \alpha_n \ket{\beta_n} </me> 
<p> The probability that <m>\ket{\psi}</m> will collapse into the basis state <m>\ket{\beta_i}</m> is given by <m>\alpha_i^2</m>. Furthermore, since qubits are normalized </p> 
<me>\sum_{i=1}^n P(\beta_i) = \sum_{i=1}^n \alpha_i^2 = 1</me>
<exercise>
  <statement><p>For the following qubit, find the probability that it will be in the state <m>\ket{0}</m> and the probability that it will be in the state <m>\ket{1}</m> after measurement</p>
  <me> \ket{\psi} = \frac{1}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1} </me></statement>
<solution><p> The probability that <m>\ket{\psi}</m> will be in the state <m>\ket{0}</m> after measurement is 25% because <m>(\frac{1}{2})^2 = 1/4 = 0.25</m>. The probability that <m>\ket{\psi}</m> will be in the state <m>\ket{1}</m> after measurement is 75% because <m>(\frac{\sqrt{3}}{2})^2 = 3/4 = 0.75</m>. </p></solution>
</exercise>
<p> Another way to find the probability that a qubit is in any given state is to take the square of the inner product of the outcome state and the qubit. For a qubit <m>\ket{\psi}</m>, the probability that it is in the state <m>\ket{0}</m> after measurement would be <m>(\braket{0|\psi})^2</m>, the probability that it is in the state <m>\ket{1}</m> would be <m>(\braket{1|\psi})^2</m>, and the probability that it is in some mixed state <m>\ket{x}</m> would be <m>(\braket{x|\psi})^2</m>.</p>
<exercise>
  <statement><p> Using an inner product, calculate the probability that the following system is in the state <m>\ket{0}</m> and <m>\ket{1}</m>.</p>
  <me> \ket{\psi} = \frac{\sqrt{7}}{\sqrt{8}} \ket{0} + \frac{1}{\sqrt{8}} \ket{1} = \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix} </me></statement>
<solution> <me> P(0) = (\braket{0|\psi})^2 = (\begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix}+0)^2 = (\frac{\sqrt{7}}{\sqrt{8}})^2 = 7/8 = 0.875 </me> <p> The probability that <m>\ket{\psi}</m> will collapse into the state <m>\ket{0}</m> is 87.5%. </p> 
<me> P(1) = (\braket{1|\psi})^2 = (\begin{pmatrix} 0 &amp; 1 \end{pmatrix} \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix})^2 = (0+\frac{1}{\sqrt{8}})^2 = 1/8 = 0.125 </me> <p> The probability that <m>\ket{\psi}</m> will collapse into the state <m>\ket{1}</m> is 12.5%. </p> </solution></exercise>
</subsection>
</section>

<section xml:id="Computational-Bases">
  <title>Computational Bases and the Bloch Sphere</title>
  <p> So far, the only basis we have looked at for measuring a qubit is <m>(\ket{0},\ket{1})</m>, but this is not the only one. In quantum computation, we define three orthogonal basis states. For now, we will call these the X-measurement, Y-measurement, and Z-measurement, which will make sense shortly.  </p>
  <p><alert>X-Measurement</alert></p>
<p> <m>\{\ket{+},\ket{-}\}</m> are eigenstates of the Pauli X Matrix, <m>\sigma_x</m> (which is <m>\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}</m>, as we will see in <xref ref="subsec2-chap2"/>). These are called the Hadamard (transversal) basis states </p>
<me> \Big\{\ket{+}:=\frac{1}{\sqrt{2}}\Big(\ket{0}+\ket{1}\Big), \ket{-}:=\frac{1}{\sqrt{2}}\Big(\ket{0}-\ket{1}\Big) \Big\} </me>
<p><alert>Y-Measurement</alert></p>
  <p><m>\{\ket{+i},\ket{-i}\}</m> are eigenstates of the Pauli Y Matrix, <m>\sigma_y</m> (which is <m>\begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix}</m>, as we will see in <xref ref="subsec3-chap2"/>).These are called the Longitudinal (Left-Right) basis states</p>
<me> \Big\{\ket{+i}:=\frac{1}{\sqrt{2}}\Big(\ket{0}+i\ket{1}\Big), \ket{-i}:=\frac{1}{\sqrt{2}}\Big(\ket{0}-i\ket{1}\Big) \Big\} </me>
<p><alert>Z-Measurement</alert></p> 
<p> <m>\{\ket{0},\ket{1}\}</m> are eigenstates of the Pauli Z Matrix, <m>\sigma_z</m> (which is <m>\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}</m>, as we will see in <xref ref="subsec4-chap2"/>). These are called the Computational basis states </p> 
<me> \Big\{\ket{0}:=\begin{pmatrix}1\\0\end{pmatrix},\ket{1}:=\begin{pmatrix}0\\1\end{pmatrix},\Big\} </me>
<exercise>
  <title>A state in the Hadamard basis</title>
    <statement><p> <m>\ket{\psi}=\frac{1}{\sqrt{2}}\Big(\ket{0}-\ket{1}\Big)</m> is measured in the basis <m>\{\ket{+},\ket{-}\}.</m> Calculate <m>P(+)</m> and <m>P(-).</m> </p></statement>
    <solution> <me> P(+) = (\braket{+|\psi})^2 = (\frac{1}{\sqrt{2}}(\bra{0} + \bra{1}) \cdot \frac{1}{\sqrt{2}}(\ket{0}-\ket{1}))^2 = (\frac{1}{2}(\braket{0|0} - \braket{0|1} + \braket{1|0} - \braket{1|1}))^2 = (\frac{1}{2}(1 - 0 + 0 - 1))^2 = 0 </me>
    <me> P(-) = (\braket{-|\psi})^2 = (\frac{1}{\sqrt{2}}(\bra{0} - \bra{1}) \cdot \frac{1}{\sqrt{2}}(\ket{0}-\ket{1}))^2 = (\frac{1}{2}(\braket{0|0} - \braket{0|1} - \braket{1|0} + \braket{1|1}))^2 = (\frac{1}{2}(1 - 0 - 0 + 1))^2 = 1 </me> </solution>
  </exercise>
<exercise>
<title>A state in the Computational basis</title>
  <statement><p> <m>\ket{\psi}=\frac{1}{\sqrt{3}}\Big(\ket{0} +\sqrt{2}\ket{1}\Big)</m> is measured in the basis <m>\{\ket{0},\ket{1}\}.</m> Calculate <m>P(0)</m> and <m>P(1)</m>. </p></statement>
  <solution> <me> P(0) = (\braket{0|\psi})^2 = (\bra{0} \cdot \frac{1}{\sqrt{3}} (\ket{0} + \sqrt{2}\ket{1}))^2 = (\frac{1}{\sqrt{3}} \braket{0|0} + \frac{\sqrt{2}}{\sqrt{3}} \braket{0|1})^2 = (\frac{1}{\sqrt{3}} + 0)^2 = \frac{1}{3} </me>
  <me> P(1) = (\braket{1|\psi})^2 = (\bra{1} \cdot \frac{1}{\sqrt{3}} (\ket{0} + \sqrt{2}\ket{1}))^2 = (\frac{1}{\sqrt{3}} \braket{1|0} + \frac{\sqrt{2}}{\sqrt{3}} \braket{1|1})^2 = (0 + \frac{\sqrt{2}}{\sqrt{3}})^2 = \frac{2}{3} </me> </solution>
</exercise>

<subsection xml:id="subsec-Bloch-Sphere">
  <title>The Bloch Sphere</title>
  <p> Recall that any qubit can be written as <m> \ket{\psi} = \alpha \ket{0} + \beta \ket{1} </m> and <m> \alpha^2 + \beta^2 = 1 </m>. Because of this bounding property, we can say there exist some real numbers <m>\gamma, \theta, \text{ and } \phi</m> such that </p>
<me> \ket{\psi} = e^{i\gamma}(\cos(\frac{\theta}{2})\ket{0} + e^{i\phi}\sin(\frac{\theta}{2})\ket{1}) </me>
<p> Since the <m>e^{i\gamma}</m> is simply a scalar applied to the entire system, it will have no observable effect on the state of the system. This means we can ignore that term, leaving us with </p>
<me> \ket{\psi} = \cos(\frac{\theta}{2})\ket{0} + e^{i\phi}\sin(\frac{\theta}{2})\ket{1} </me>
<p> The numbers <m>\theta</m> and <m>\phi</m> define a point on the three-dimensional unit sphere, which we will here call the <term>Bloch sphere</term><idx><h>Bloch Sphere</h></idx>. The Bloch sphere exists within a Hilbert Space, <m>\mathcal{H}</m>, and provides an intuitive method for depicting qubits as vectors and visualizing the operations we can perform on them.</p>
  <figure xml:id="fig-Bloch-Sphere">
    <caption>The Bloch sphere and the representation of a qubit <m>\ket{\psi}</m> with angle <m>\phi</m> along the horizontal plane and angle <m>\theta</m> along the vertical axis, as well as the computational basis vectors.  </caption>
  <image source="BlochSphere.png" width="60%"> </image> 
  </figure>
<p> One quirk of the Bloch sphere is that any two orthogonal state vectors (qubits) are represented on the sphere not as perpendicular, but as two vectors along the same line pointing in opposite directions. Thus the x-axis is defined by <m>\ket{+}</m> in the positive direction and <m>\ket{i}</m> in the negative direction, the y-axis is defined by <m>\ket{+i}</m> in the positive and <m>\ket{-i}</m> in the negative, and the z-axis is defined by <m>\ket{0}</m> in the positive and <m>\ket{1}</m> in the negative, hence the reason for the names of these states. </p>
<p> Those familiar with multivariate calculus will know that any point on the surface of the unit sphere can be described with the vector </p>
<me> \vec{r} = \begin{pmatrix} \sin \theta \cos \phi \\ \sin \theta \sin \phi \\ \cos \theta \end{pmatrix} </me>
<p> If we choose <m>\theta = 0</m>, we obtain </p>
  <me> \vec{r} = \begin{pmatrix}0\\0\\1\end{pmatrix} </me>
<p> which corresponds to <m>\ket{0}</m> on the Block sphere.</p>
<p> If we choose <m>\theta=\pi</m>. we obtain </p>
<me> \vec{r} = \begin{pmatrix}0\\0\\-1\end{pmatrix} </me>
<p> which corresponds to <m>\ket{1}</m> on the Block sphere. This is our reason for choosing to use <m>\frac{\theta}{2}</m> as an angle in our definition of the block instead of just using <m>\theta</m>. When <m>\theta=0</m>,</p>
<me> \ket{\psi} = \cos(0)\ket{0} + e^{i\phi}\sin(0)\ket{1} = \ket{0}</me>
<p> and when <m>\theta = \pi</m> and <m>\phi = 0</m>, </p>
  <me> \ket{\psi} = \cos(\frac{\pi}{2})\ket{0} + e^{0}\sin(\frac{\pi}{2})\ket{1} = \ket{1}</me>
<exercise>
  <statement>
    <p> Using the vector equation for points on the surface of the unit sphere, consider the following values: </p>
  <p>1. <m>\theta = \frac{\pi}{2}</m> and <m>\phi =0.</m> Obtain <m> \vec{r}.</m></p>
<p>2. <m>\theta = \frac{\pi}{2}</m> and <m>\phi =\pi.</m> Obtain <m> \vec{r}.</m></p>
 <p>3. <m>\theta = \frac{\pi}{2}</m> and <m>\phi =\frac{\pi}{2}.</m> Obtain <m>\vec{r}.</m> </p>
<p>4. <m>\theta = \frac{\pi}{2}</m> and <m>\phi =\frac{3\pi}{2}.</m> Obtain <m>\vec{r}.</m> </p> </statement>
</exercise>
<p>The Bloch sphere is called a projective sphere because the states of our quantum system are rays in the Hilbert space<m>\mathcal{H}</m>, and we would prefer to visualize vectors as points, not rays. Going back to the underlying <m>\mathbb{C}^{n}</m> we collapse the ray that represents a quantum state onto the surface of an <m>n</m>-dimensional sphere. We are projecting all those representatives onto a single point on the complex n-sphere. Notice that each point on that sphere still has infinitely many representations impossible to picture due to the potential scalar factor <m>e^{i \gamma}</m>, that we left out of our equation. </p>
<p> The Pauli gates <m>X, Y,</m> and <m>Z</m> (which we will see in <xref ref="sec2-chap2"/>) correspond to rotations about the <m>x-, y-</m> and <m>z-</m>axes of the Bloch sphere </p>
</subsection>
</section>

<section xml:id="Operators">
  <title>Operators</title>
  <p>In physics, things we are able to measure are called <term>observables</term><idx><h>Observables</h></idx>. Examples of observables are things like position, momentum, and energy, among many others. Any observables that relate to the quantum state of a particle have a corresponding <term>operator</term><idx><h>Operator</h></idx>. Operators are used to map vectors from one vector space onto another. An operator <m>A</m> that maps from a vector space <m>V</m> to a vector space <m>W</m> could be written as <m> A: V \rightarrow W</m>. In quantum computation, operators act on kets from the left side and on bras from the right side. For a ket <m>\psi</m> and a bra <m>\phi</m>, an operator <m>A</m> would act on them as follows</p>
<me> A \cdot \ket{\psi} = A\ket{\psi}, \bra{\phi} \cdot A = \bra{\phi}A </me> 
<p> Consider that <m>A</m> is not a scalar, but rather an operator maps between vector spaces, which means that these equations do not just represent basic multiplication. In quantum theory, operators can be represented by matrices, and tools like matrix multiplication and matrix addition (see <xref ref="linear-algebra"/>) can be used to perform operations on qubits. </p> 
<subsection xml:id="subsec-Hermitian-Operators">
  <title>Hermitian Operators</title>
  <p> Recall from <xref ref="subsec-Bra-Vectors-and-the-Inner-Product"/> that the adjoint operation represented by the <m>\dagger</m> symbol consisted of transposing and complex conjugating a vector. This same operation can be performed on an operator (note that operations and operators are two different things). An operator <m>X</m> is <term>Hermitian</term><idx><h>Operator</h><h>Hermitian Operator</h></idx> (also referred to as adjoin) if it satisfies the following property: </p> 
  <me> X = X^{\dagger} </me>
<p>Other properties of Hermitian operators are </p>
<p><alert> Noncommutative </alert></p>
<me> XY \neq YX </me>
<p><alert> Associative (multiplicative) </alert></p>
<me> XYZ = (XY)Z = X(YZ) </me>
<p><alert> Hermitian Product </alert></p>
<me> (XY)^{\dagger} = Y^{\dagger}X^{\dagger} </me>
<p><alert> Linear </alert>  An operator <m>L</m> takes a vector <m>\vec{v}</m> and transforms it into a new vector <m>L\vec{v}</m>. If <m>L</m> is a linear operator, then </p>
<me> L(\alpha \vec{v} + \beta \vec{w}) = \alpha L \vec{v} + \beta L \vec{w} </me>
<p> where <m>\alpha, \beta \in \mathbb{C}</m> and <m>\vec{v},\vec{w} \in \mathcal{H}</m> </p> 
<exercise>
  <statement>
    <p>Which of the following operators are Hermitian?</p>
    <me>1. \ \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} </me>
    <me>2. \ \begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix} </me>
    <me>3. \ \begin{pmatrix} -i &amp; 0 \\ 0 &amp; i \end{pmatrix} </me>
    <me>4. \ \begin{pmatrix} 1 &amp; 1+i \\ 1-i &amp; 1 \end{pmatrix} </me>
  </statement>
  <solution>
    <p>1.</p> 
    <me> \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}^{\dagger} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} </me> 
    <p> Since this matrix equals its adjoint, it is Hermitian </p>
    <p>2.</p> 
    <me> \begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix}^{\dagger} = \begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix} </me> 
    <p> Since this matrix equals its adjoint, it is Hermitian </p>
    <p>3.</p> 
    <me> \begin{pmatrix} -i &amp; 0 \\ 0 &amp; i \end{pmatrix}^{\dagger} = \begin{pmatrix} i &amp; 0 \\ 0 &amp; -i \end{pmatrix} </me> 
    <p> Since this matrix does not equal its adjoint, it is not Hermitian </p>
    <p>4.</p> 
    <me> \begin{pmatrix} 1 &amp; 1+i \\ 1-i &amp; 1 \end{pmatrix}^{\dagger} = \begin{pmatrix} 1 &amp; 1+i \\ 1-i &amp; 1 \end{pmatrix} </me> 
    <p> Since this matrix equals its adjoint, it is Hermitian </p>
  </solution>
</exercise>
</subsection>
<subsection xml:id="subsec-Unitary-Operators">
  <title>Unitary Operators</title>
<p> Recall that for a qubit </p><me>\ket{\psi}=\alpha \ket{0} + \beta \ket{1}</me><p> the property <m>\alpha^2 + \beta^2 = 1</m> must hold. After an operator <m>U</m> is applied to this qubit, we get a new state defined by </p>
<me> U\ket{\psi} = \ket{\psi'} = \alpha'\ket{0} + \beta'\ket{1} </me>
<p> If the property that <m>\alpha'^2 + \beta'^2 = 1</m> still holds, then the operator <m>U</m> is <term>unitary</term><idx><h>Operator</h><h>Unitary Operator</h></idx>. Unitary operators map qubits between Hilbert spaces. A unitary operator ,<m>U</m> could be defined as <m>U:\mathcal{H} \rightarrow \mathcal{H}</m>. A defining property of unitary operators is </p>
<me> U U^{\dagger} = U^{\dagger} U = I </me>
<p> where <m>I</m> is the identity matrix. </p>
<p> Unitary operations performed on a qubit are reversible. This means that for an operator <m>U</m> that acts on a qubit <m>\ket{\psi}</m> such that <m>U\ket{\psi} = \ket{\psi'}</m>, there exists some operator <m>U^{-1}</m> such that <m>U^{-1}\ket{\psi'} = \ket{\psi}</m>. This <m>U^{-1}</m> is called the inverse of <m>U</m>. Inverse matrices have the property that</p>
<me> UU^{-1} = U^{-1}U = I </me>
<p> This means that for Unitary operators, the following property holds: </p>
<me> U^{-1} = U^{\dagger} </me>
</subsection>
<subsection xml:id="subsec-Additional-Properties">
  <title>Additional Properties and Other</title>
  <p> Recall from <xref ref="Operators"/> that operators work on a ket from the left side and on a bra from the right side. Additionally, recall </p>
  <me> \ket{\psi}^{\dagger} = \bra{psi} </me>
  <p> For an operator <m>A</m> acting on a ket <m>\ket{\psi}</m>, the adjoint is </p>
  <me> (A \ket{\psi})^{\dagger} = \bra{\psi}A^{\dagger} </me>
  <p> and <m>A^{\dagger}</m> is another operator. </p> 
  <p></p>
  <p> That is the extent to which we will discuss operators in this webbook, but readers wishing to extend their knowledge should go to this document <url href="https://tinyurl.com/yzy4ycj5"></url></p>
<p></p>
<p> Below is pictured a circuit diagram (which we will delve deeper into in <xref ref="sec3-chap2"/>) representing the action on a single Qbit of the 1-Qbit gate <m>U</m>. Initially the Qbit is described by the input state <m>\ket{\psi}</m> on the left of the line. The line (wire) represents the subsequent operation on the Qbit. After emerging from the box representing the operator <m>U</m>, the Qbit is described by the final state <m>U \ket{\psi}.</m></p>
<image source="circuitdiagram.png" width="60%"> </image>
</subsection>
</section>

<section xml:id="sec-Systems-of-Multiple-Qubits">
  <title>Systems of Multiple Qubits</title>
  <p> Any system of <m>n</m> qubits will have <m>2^n</m> basis states. As we have seen already, in a one-qubit system, the two basis states are <m>\ket{0}</m> and <m>\ket{1}</m>. In a two-qubit system, the four basis states are <m>\ket{00}</m>, <m>\ket{01}</m>, <m>\ket{10}</m>, and <m>\ket{11}</m>. These four basis states each have vector representations, which are found by using the <term>tensor product</term><idx><h>Tensor Product</h></idx> </p>
  <subsection xml:id="subsec-Tensor-Product">
    <title>Tensor Product</title>
    <p> The tensor product (represented by a <m>\otimes</m> symbol) is an operation between two matrices (or vectors) that multiplies each entry in the matrix on the left by the matrix on the right. Thus the tensor product between a <m>m\times n</m> matrix and a <m>p \times q</m> matrix will be a <m>mp \times nq </m>, as shown below. </p>
<me> A \otimes B = \begin{pmatrix} A_{11}B &amp; A_{12}B &amp; A{13}B &amp; \\ A_{21}B &amp; A_{22}B &amp; A_{23}B \\ A_{31}B &amp; A_{32}B &amp; A_{33}B \end{pmatrix} </me>
    <p>where each <m>A_{ij}</m> is the entry in the <m>i</m>-th row and <m>j</m>-th column of <m>A</m>. </p>
    <p> For scalars <m>\alpha,\beta \in \mathbb{C}</m> and vectors <m>\ket{\psi},\ket{\phi},\ket{\rho}\in\mathcal{H}</m> the tensor product can be distributed as follows </p>
    <me> \ket{\psi} \otimes (\alpha \ket{\phi} + \beta \ket{\rho}) = \alpha(\ket{\psi} \otimes \ket{\phi}) + \beta(\ket{\psi} \otimes \ket{\rho}) = \alpha \ket{\psi\phi} + \beta \ket{\psi\rho} </me>
    <exercise>
      <statement><p>Computer the following tensor products</p>
      <me>1. \ \begin{pmatrix}1 \\ 2 \end{pmatrix} \otimes \begin{pmatrix}4 \\ 8 \end{pmatrix} </me> 
  <me>2. \ \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} \otimes \begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix}</me></statement>
  <solution> 
    <me>1. \ \begin{pmatrix}1 \\ 2 \end{pmatrix} \otimes \begin{pmatrix}4 \\ 8 \end{pmatrix} = \begin{pmatrix} 1 \times 4 \\ 1 \times 8 \\ 2 \times 4 \\ 2 \times 8 \end{pmatrix} =  \begin{pmatrix} 4 \\ 8 \\ 8 \\ 16 \end{pmatrix}</me>
    <me>2. \ \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 3 \end{pmatrix} \otimes \begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} = \begin{pmatrix} 2\begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} &amp; 1\begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} \\ 1\begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} &amp; 3\begin{pmatrix} 4 &amp; 1 \\ 0 &amp; 2 \end{pmatrix} \end{pmatrix} = \begin{pmatrix} 8 &amp; 2 &amp; 4 &amp; 1 \\ 0 &amp; 4 &amp; 0 &amp; 2 \\ 4 &amp; 1 &amp; 12 &amp; 3 \\ 0 &amp; 2 &amp; 0 &amp; 6 \end{pmatrix} </me></solution>
    </exercise>
    
  </subsection>

  <subsection xml:id="subsec-Systems-of-Multiple-Qubits">
    <title>Systems of Multiple Qubits</title>
    <p> The vector representation of a two qubit system <m>\ket{\psi \phi}</m> would be found by the tensor product <m>\ket{\psi} \otimes \ket{\phi}</m>. Thus the four basis states of a two qubit system are defined as: </p>
    <me> \ket{00} = \ket{0} \otimes \ket{0} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix}  </me>
    <me> \ket{01} = \ket{0} \otimes \ket{1} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}  </me>
    <me> \ket{10} = \ket{1} \otimes \ket{0} = \begin{pmatrix} 0 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix}  </me>
    <me> \ket{11} = \ket{1} \otimes \ket{1} = \begin{pmatrix} 0 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}  </me>
    <p> This concept can be generalized to systems of any number of qubits. A system of two qubits is referred to as <term>bipartite</term><idx><h>Bipartite</h></idx> or composite.      </p>
    
    
    <p> The tensor product can also be applied to vector spaces. The tensor product of a vector that exists within a space <m>V</m> and a vector that exists within a space <m>W</m> would exist within the space <m>V \otimes W</m>. Thus, a system of two qubits would exist within the space <m> \mathcal{H} \otimes \mathcal{H}</m> and a system of <m>n</m> qubits exists within the space <m>\mathcal{H}^{\otimes n}</m>, where the <m>\otimes n</m> superscript means taking the tensor product of <m>\mathcal{H}</m> with itself <m>n</m> times. Similarly, <m>\ket{\psi}^{\otimes n}</m> would represent the state <m>\ket{\psi}</m> tensored with itself <m>n</m> times. Suppose the space <m>V</m> is <m>n</m> dimensional the space <m>W</m> is <m>m</m> dimensional, then the vector space <m>V \otimes W</m> would be <m>n\cdot m</m> dimensional. </p>
    <example>
      <me> \ket{\psi}^{\otimes3} = \ket{\psi} \otimes \ket{\psi} \otimes \ket{\psi}  </me>
    </example>
    <p>Suppose, we have two vector spaces <m>V</m> and <m>W,</m> and we want to know what kind of operator act on the space <m>V\otimes W</m>. If <m>A</m> is an operator on the space <m>V</m> and <m>B</m> is an operator on the space <m>W</m>, then <m>A \otimes B</m> is an operator on the space <m>V \otimes W</m>.  An operator <m>A\otimes B</m> is linear if </p>
    <me>\big(A\otimes B\big) \big(\ket{v}\otimes \ket{w}\big) = A\ket{v}\otimes B \ket{w} </me>
    <p> For <m>\ket{v} \in V</m> and <m>\ket{w} \in W</m>. </p>
    <exercise>
    <statement><p>For the two qubit system <m>\ket{\psi}</m> compute the probabilities that it will collapse into each the basis states <m>\ket{00}</m>, <m>\ket{01}</m>, <m>\ket{10}</m>, and <m>\ket{11}</m></p> 
      <me> \ket{\psi} = \frac{1}{2\sqrt{2}} \ket{00} + \frac{\sqrt{3}}{2\sqrt{2}} \ket{01} + \frac{1}{2}\ket{10} + \frac{1}{2} \ket{11}</me></statement>
  <solution> <me> P(00) = \frac{1}{8} </me> 
    <me> P(01) = \frac{3}{8} </me> 
    <me> P(10) = \frac{1}{4} </me> 
    <me> P(11) = \frac{1}{4} </me> </solution>
    </exercise>
    <exercise>
      <statement><p> Compute <m>\ket{\psi}^{\otimes 4}</m> </p> <me> \ket{\psi} = \frac{1}{\sqrt{3}}\ket{0} + \frac{\sqrt{2}}{\sqrt{3}}\ket{1} </me></statement>
    <hint> <me> \ket{\psi} = \begin{pmatrix} \frac{1}{\sqrt{3}} \\ \frac{\sqrt{2}}{\sqrt{3}} \end{pmatrix} = \frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ \sqrt{2} \end{pmatrix}</me> </hint>
  <solution> <me>\ket{\psi}^{\otimes 4} = \ket{\psi} \otimes  \ket{\psi} \otimes  \ket{\psi} \otimes \ket{\psi} =  \frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ \sqrt{2} \end{pmatrix} \otimes \frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ \sqrt{2} \end{pmatrix} \otimes \frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ \sqrt{2} \end{pmatrix} \otimes \frac{1}{\sqrt{3}} \begin{pmatrix} 1 \\ \sqrt{2} \end{pmatrix} </me> 
  <me> = \frac{1}{3} \begin{pmatrix} 1 \\ \sqrt{2} \\ \sqrt{2} \\ 2 \end{pmatrix} \otimes \frac{1}{3} \begin{pmatrix} 1 \\ \sqrt{2} \\ \sqrt{2} \\ 2 \end{pmatrix} = \frac{1}{9} \begin{pmatrix} 1 \\ \sqrt{2} \\ \sqrt{2} \\ 2 \\ \sqrt{2} \\ 2 \\ 2 \\ 2\sqrt{2} \\ \sqrt{2} \\ 2 \\ 2 \\ 2\sqrt{2} \\ 2 \\ 2\sqrt{2} \\ 2\sqrt{2} \\ 4 \end{pmatrix} </me></solution>
  </exercise>
  </subsection>
</section>

<section xml:id="sec-Operations">
  <title>Operations in Quantum Computation</title>
<subsection xml:id="subsec-Outer-Product">
  <title>Outer Product</title>
 <p> The tensor product is only one of many operations on qubits. In <xref ref="linear-algebra"/> we described how the inner product could be represented as the product of a vector's transpose and the vector. Since an <m>n</m> dimensional vector transpose is <m>1\times n</m> and the vector is <m>n\times1</m>, the resulting product is <m>1\times1</m>, which is functionally equivalent to a scalar. If we were to reverse the order of the vector multiplication and multiply a column vector on the left and a row vector on the right, we would multiply an <m>n\times1</m> vector by an <m>1\times m</m> vector to produce an <m>n\times m</m> matrix. More specifically, we want to multiply a vector on the left by the vectors adjoint (complex conjugated transpose) on the right. This operation is known as the <term>outer product</term><idx><h>Outer Product</h></idx>. The outer product between two qubits <m>\ket{\psi}</m> and <m>\ket{\phi}</m> is shown below. </p> 
<me> \ket{\psi} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}, \ket{\phi} = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{pmatrix} </me>
  <me> \ket{\psi}\bra{\phi} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}  \begin{pmatrix} \beta_1^* &amp; \beta_2^* &amp; \ldots &amp; \beta_m^* \end{pmatrix} = \begin{pmatrix} \alpha_1 \beta_1^* &amp; \alpha_1 \beta_2^* &amp; \ldots &amp;\alpha_1\beta_m^*\\ \alpha_2 \beta_1^* &amp; \alpha_2 \beta_2^* &amp;\ldots &amp;\alpha_2\beta_m^*\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \alpha_n \beta_1^* &amp; \alpha_n \beta_2^* &amp;\ldots &amp;\alpha_n\beta_m^*\\ \end{pmatrix} </me>
</subsection>
<subsection xml:id="subsec-Completeness-Relation">
  <title>Completeness Relation</title>
 <p> If a set of basis vectors for a quantum system <m>\{ \ket{\mathcal{B}_1},\ket{\mathcal{B}_2},\ldots,\ket{\mathcal{B}_n} \}</m> have the property that the sum of the outer products of each basis vector with itself is equal to the identity matrix:</p>
 <me> \sum_{i=1}^{n} \ket{\mathcal{B}_i}\bra{\mathcal{B}_i}= I </me>
  <p> Then that set is said to have a <term>completeness relation</term><idx><h>Completeness Relation</h></idx>. In general, orthonormal bases will have a completeness relation. Since the completeness relation is the same as the identity operator, we can use it on any vector expression without changing its value. </p>
<p> Suppose <m>\ket{\psi}</m> is a vector in a space <m>V</m> and <m> \{ \ket{\phi_1},\ket{\phi_2},\ldots,\ket{\phi_n}  \}</m> is an orthonormal basis of <m>V</m> with a completeness relation. We can then use the completeness relation as follows </p>
  <me> \begin{split} \ket{\psi} &amp;= I \ket{\psi} \\ &amp;= \sum_{i=1}^n \ket{\phi_i} \bra{\phi_i} \ket{\psi} \\ &amp;= \ket{\phi_1} \bra{\phi_1} \ket{\psi} + \ket{\phi_2} \bra{\phi_2} \ket{\psi} + \ldots + \ket{\phi_n} \bra{\phi_n} \ket{\psi} \\ &amp;= \alpha_1 \ket{\phi_1} + \alpha_2 \ket{\phi_2} + \ldots + \alpha_n \ket{\phi_n} \end{split} </me>
<p> Where each <m> \alpha_j = \braket{\phi_j | \psi} </m> and represents the component of the vector <m>\ket{\psi}</m> that is in the direction of the basis <m>\ket{\phi_j}</m>. Thus the completeness relation can be used to decompose a vector into its basis elements. </p>
</subsection>
  <subsection xml:id="subsec-Inner-Product">
    <title>Inner Product</title>
    <p> Here we will redefine the inner product in Bra-Ket notation and describe some additional properties. For two qubits <m>\ket{a}</m> and <m>\ket{b}</m> </p>
    <me> \ket{a}= \begin{pmatrix}a_{1} \\a_{2} \\\vdots \\a_{n}\end{pmatrix}, \ket{b}=\begin{pmatrix}b_{1} \\b_{2} \\\vdots \\b_{n} \end{pmatrix}  </me>
  <p>their inner product</p>
  <me> \braket{a | b} = \sum_{k=1}^{n} a_{k} b_{k} </me> 
  <p> We now define </p> 
  <me> \delta_{k j}= \Bigg \{ \begin{array}{ll} 1, &amp; \text { if } k=j \\ 0, &amp; \text { if } k\neq j \end{array} </me>
<p> We call <m>\delta_{k j}</m>, the Kronecker delta. It is the mathematical way to express anything that is equal to 0 unless the index <m>k=j</m>, in which case it is 1. For an orthonormal basis <m> \{ \ket{\beta_1}, \ket{\beta_n}, \ldots, \ket{\beta_n} \}</m> we have the property </p>  
<me> \braket{\beta_k | \beta_j} = \delta_{kj} </me>
<p> That is, for any two vectors in the basis, their inner product is 0, unless the two vectors are the same, in which case their inner product is 1. </p> 
  <p> For <m>\alpha \in \mathbb{C}</m> and <m>\ket{\psi},\ket{\phi},\ket{\rho}\in\mathcal{H}</m>, the inner product has the following properties: </p>
  <me> \begin{split} &amp; \text{1. Distributive Law: } \bra{\psi}(\ket{\phi} + \ket{\rho}) = \braket{\psi | \phi} + \braket{\psi | \rho}  
 \\ &amp; \text{2. Associative Law: } \bra{\psi} (\alpha \ket{\phi}) = \alpha \braket{ \psi | \phi} 
  \\ &amp; \text{3. Hermitian Symmetry: } \braket{\psi | \phi} = \braket{\phi | \psi}^{*} 
  \\ &amp; \text{4. Definite Form: } \braket{\psi | \psi} \geq 0 \text{ and } \braket{\psi | \psi} = 0 \text{ only if } \ket{\psi} = \vec{0} \end{split}</me>
</subsection>
<subsection xml:id="subsec-Diagonalization">
  <title>Diagonalization</title>
  <p> For a vector space <m>V</m> with a basis <m>\{ \ket{\beta_1}, \ket{\beta_2}, \ldots, \ket{\beta_n} \}</m>, A diagonal representation for an operator <m>A</m> that acts on the space would be  
    <me>A=\sum_{i} \lambda_{i}\ket{\beta_i}\bra{\beta_i}</me> 
    where the <m>\lambda_i</m> are eigenvalues that correspond to the basis state <m>\beta_i</m>. An operator is said to be <term>diagonalizable</term><idx><h>Diagonalizable</h></idx> if it has a diagonal representation. As an example of a diagonal representation, note that the Pauli <m>Z</m> matrix (see <xref ref="subsec4-chap2"/>) may be written</p> 
    <me> Z = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{pmatrix} - \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = \ket{0}\bra{0} - \ket{1}\bra{1} = \sum_{i=1}^{2} \lambda_i \ket{\beta_i}\bra{\beta_i}</me>
<p> where <m>\lambda_1 = 1, \lambda_2 = -1, \beta_1 = \ket{0}, \text{ and }\beta_2  = \ket{1}</m>. Diagonal representations are sometimes also known as orthonormal decompositions.</p> 
  </subsection>
  <subsection xml:id="subsec-Density-Operator">
    <title>Density Operator</title>
    <p> Any basis state for a vector space can also be expressed with a <term>density operator</term><idx><h>Density Operator</h></idx> that provides us with another method to study the state of the entire system. For a vector space with a basis <m>\{ \ket{\psi_1},\ket{\psi_2},\ldots,\ket{\psi_n} \}</m>, the density operators for its basis states are given by  </p>
    <me> \rho_i = \ket{\psi_i}\bra{\psi_i} </me>
    <p> This means that the density operator <m>\rho</m> for any basis state is equal to the outer product of that state with itself. These density operators for states have the following properties:</p>
<me> \begin{split} &amp; \text{1. Idempotent: } \rho^2 = \ket{\psi}\braket{\psi | \psi} \bra{\psi} = \ket{\psi}\bra{\psi} = \rho
 \\ &amp; \text{2. Trace(the sum of the diagonals): For an } n \times n \text{ density operator: } \\ &amp; Tr(\rho) = \sum_{i,j=1}^n \rho_{i,j} = 1 \text{ (where } \rho_{i,j} \text{ represents the entry in the i-th row and j-th column of }\rho)
 \\ &amp; \text{3. Hermiticity: } \rho^{\dagger} = (\ket{\psi}\bra{\psi})^{\dagger} = \ket{\psi}\bra{\psi} = \rho
 \\ &amp; \text{4. Positive Semi-Definite: For a state }\ket{\phi}, \bra{\phi}\rho\ket{\phi} = \braket{\phi|\psi} \braket{\psi|\phi} = \braket{\phi|\psi}^2 \geq 0 \end{split} </me> 
 <p> For a state </p>
 <me> \ket{\psi} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix} </me> 
 <p> The density operator would be </p>
 <me> \rho = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix} \begin{pmatrix} \alpha_1^* &amp; \alpha_2^* &amp; \ldots &amp; \alpha_n^* \end{pmatrix} = \begin{pmatrix} \alpha_1\alpha_1^* &amp; \alpha_1\alpha_2^* &amp; \ldots &amp; \alpha_1\alpha_n^* \\  \alpha_2\alpha_1^* &amp; \alpha_2\alpha_2^* &amp; \ldots &amp; \alpha_2\alpha_n^*  \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \alpha_n\alpha_1^* &amp; \alpha_n\alpha_2^* &amp; \ldots &amp; \alpha_n\alpha_n^* \end{pmatrix} </me>   
 <exercise>
  <statement> <p> Find the density operator for the following state </p> 
    <me> \ket{\psi} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11}) </me> </statement>
    <hint> <me> \ket{\psi} = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\0\\0\\1 \end{pmatrix} </me> </hint> 
    <solution> <me> \rho = \frac{1}{\sqrt{2}}\begin{pmatrix} 1\\0\\0\\1 \end{pmatrix} \frac{1}{\sqrt{2}}\begin{pmatrix} 1&amp;0&amp;0&amp;1 \end{pmatrix} = \frac{1}{2}\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} </me></solution> 
 </exercise>
 <p> Now suppose we want to find a density operator for an entire system. The system exists within a vector space with a basis <m>\{ \ket{\beta_1},\ket{\beta_2},\ldots,\ket{\beta_n} \}</m> and has a probability <m>p_i</m> of being in the state <m>\ket{\beta_i}</m> after measurement. The density operator for the entire system is defined by </p> 
<me> \rho = \sum_{i=1}^n p_i \rho_i = \sum_{i=1}^n p_i \ket{\beta_i}\bra{\beta_i} </me> 
<p> The density operator for the system has the same properties as the density operator for individual states: Idempotent, Trace=1, Hermiticity, and Positive Semi-Definite.</p>
    </subsection>

  <subsection xml:id="subsec-The-Commutator">
    <title>The Commutator</title>
    <p> Remember that matrix multiplication is generally not commutative, i.e. for two matrices <m>A</m> and <m>B</m>, <m>AB \neq BA</m>. However, there are exceptions to this generality. The <term>commutator</term><idx><h>Commutator</h></idx> between two operators <m>A</m> and <m>B</m> is defined to be </p> 
<me> [A, B] \equiv A B-B A </me> 
<p> If <m>[A, B]=0</m>, that is, <m>A B=B A</m>, then we say <m>A</m> commutes with <m>B</m>. Similarly, the <term>anti-commutator</term><idx><h>Anti-Commutator</h></idx> of two operators <m>A</m> and <m>B</m> is defined by</p> 
<me> \{A, B\} \equiv A B+B A </me>
<p> we say <m>A</m> anti-commutes with <m>B</m> if <m>\{A, B\}=0</m>, that is <m>AB = -BA</m>. It turns out that many important properties of pairs of operators can be deduced from their commutator and anti-commutator. </p>
  </subsection>
</section>


</chapter>