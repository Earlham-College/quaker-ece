<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="chap1-chapter" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Basic Concepts of Quantum Computation</title>
  
  <xi:include href="chap1-intro.ptx" />
 

  <section xml:id="Classical-Physics">
    <title> Classical Physics </title>
    <p> Classical physics, a cornerstone of scientific understanding, encompasses several branches that describe the physical phenomena of the universe, laying the groundwork for modern science.  Optics, electromagnetism, nuclear physics, astrophysics, and thermodynamics are pivotal fields that have evolved through centuries of inquiry and discovery. Optics, the study of light and its interactions with matter, dates back to ancient civilizations. The Greeks, particularly Euclid and Ptolemy, made early contributions with theories on reflection and refraction.  </p>
    <p> In the 17th century, Johannes Kepler elucidated the principles of lenses and vision, while Galileo's telescopic observations expanded our understanding of the cosmos. Isaac Newton's corpuscular theory of light and Christian Huygens' wave theory were significant milestones, reconciled in the 19th century by experiments demonstrating the wave nature of light. </p>
    <p> Electromagnetism, dealing with electric and magnetic fields and their interactions, began in the 18th century with static electricity and magnetism as separate phenomena. Benjamin Franklin's experiments with lightning and Luigi Galvani's work on bioelectricity were early milestones. The unification of electricity and magnetism into a single theory was achieved by James Clerk Maxwell in the mid-19th century, whose equations predicted the existence of electromagnetic waves, leading to Heinrich Hertz's discovery of radio waves. </p>
    <p> Nuclear physics, focusing on atomic nuclei, emerged in the early 20th century, building on classical foundations. The discovery of radioactivity by Henri Becquerel and the subsequent work of Marie and Pierre Curie opened new frontiers. Ernest Rutherford's gold foil experiment, which revealed the nucleus of an atom, was pivotal. Early developments in nuclear decay laws and the identification of radiation types owe much to classical methodologies. </p>
    <p> Astrophysics, applying physics to celestial objects and phenomena, has ancient origins in observational astronomy. Early contributions by Hipparchus and Ptolemy laid the groundwork, but the Copernican Revolution in the 16th century and Johannes Kepler's laws of planetary motion advanced the field. Newton's law of universal gravitation unified terrestrial and celestial mechanics under a single theory, revealing that the force that moved the stars was the same one that held us to the ground: gravity. </p>
    <p> Thermodynamics, the study of heat, work, and energy, emerged from the Industrial Revolution's practical needs. Sadi Carnot's work on steam engines laid the foundation for the field. The first and second laws of thermodynamics, formulated by Rudolf Clausius and William Thomson (Lord Kelvin), established principles governing energy conservation and entropy, profoundly influencing scientific thought and engineering practice. </p>
    <p> Classical physics is crucial academically as it forms the foundational bedrock upon which modern physics and other scientific disciplines are built. It introduces students to fundamental concepts such as Newtonian mechanics, thermodynamics, and electromagnetism, cultivating critical thinking and problem-solving skills. Mastery of classical physics fosters an appreciation for the scientific method and the historical context of discoveries, illustrating the interplay of theoretical advances and experimental validations. Serving as an indispensable educational foundation, classical physics equips students with the knowledge and skills necessary for careers in science, engineering, and technology, continuing to inspire future innovations and discoveries. </p>
</section>

<section xml:id="Newtonian-Mechanics">
  <title> Newtonian Mechanics</title>
  <p> Many of the concepts behind Classical Physics are motivated by Newtonian Mechanics, which is derived from Newton’s three laws of motion. These laws provide a framework for determining how an object will move when acted upon (or not acted upon) by external forces. Newton’s three laws were first described in his <i>Philosophiæ Naturalis Principia Mathematica</i>, which is widely considered to be one of the most important works in the history of physics. <!-- \cite{newton1850newton} -->  </p>
  <p> Newton’s first law, put simply, is that in the absence of external forces, an object at rest will stay at rest and an object in motion will stay in motion. Furthermore, without outside interference, an object in motion will travel at a constant speed in a straight line. In <i>Principia</i>, Newton defines inertia as the property of matter to preserve its current state and resist attempts to change it. <!-- \cite{newton1850newton} --> Thus, the first law of motion establishes that all objects have the property of inertia and resist changes to their state of rest or motion. </p>
  <p> The second law of motion is often abbreviated by the equation <m> \sum \vec{F} = m\vec{a}. </m> This equation tells us that the acceleration (<m>\vec{a}</m>) of an object is directly proportional to the net force (<m>\sum \vec{F}</m>) acting upon it and is inversely proportional to the mass (<m>m</m>) of the object. Mass can be thought of as a numerical measure of inertia, so the second law relates to the first law in that objects with a lot of mass, i.e. objects with a lot of inertia, require greater forces to accelerate. Furthermore, if we look at the case where there are no forces acting upon an object, i.e. <m>\sum \vec{F}=0</m>, we know that <m>m\vec{a}=0</m> as well. Since the mass of an object can never be <m>0</m>, when there are no forces acting upon an object, it must have an acceleration of <m>0</m>. In the absence of acceleration, an object at rest will stay at rest and an object in motion will continue to travel at a constant speed. </p>
  <p> Newton’s third law states that when two objects interact, the force exerted by the first object on the second is equal in magnitude and opposite in direction to the force exerted by the second object on the first. Simply put, every action has an equal and opposite reaction.  </p>
  <p> The three laws of motion can be seen in the example of a rocket launching into space. As the thrusters begin to fire, the rocket does not immediately begin to move. Since the rocket begins at rest, its inertial properties resist changing to a state of motion, which showcases the first law. Once the rocket begins to lift off the ground, it appears to be moving upwards slowly. Since the rocket is very heavy, it has a lot of mass, and since acceleration is inversely proportional to mass by Newton’s second law, it thus accelerates slowly. The third law of motion can be seen in the fact that the rocket engines are pushing down, which causes the equal and opposite reaction of the rocket to move upwards.  </p>
  <activity>
    <video xml:id="yt-list-vars" youtube="fhYMh6KTJMQ" width="65%" margins="35%" />
  </activity>
</section>

<section xml:id="Classical-Computing">
  <title> Classical Computing </title>

  <p> Before diving into the basics of quantum computing, we must first discuss how computing is done classically. On non-quantum computers, information is stored in strings of bits, where each bit is either a 0 or a 1. These strings of bits represent numbers in binary and they provide computers with instructions on what to do. The notable difference on a quantum computer is that quantum bits (qubits) exist in a <term>superposition</term><idx><h>Superposition</h></idx> of states that allows them to be both a 0 and a 1 until they are measured and this superposition collapses. This unique property of qubits allows quantum computers to perform multiple processes simultaneously. </p>
  <p>When describing the state of a qubit, instead of writing 0 and 1, we use <m>\ket{0}</m> and <m>\ket{1}</m>. These quantum states belong to a <term>vector space</term><idx><h>Vector</h><h>Vector Space</h></idx>, which means that multiplying states by a constant coefficient and adding states together will result in another valid quantum state. This is how a superposition is formed, by creating a linear combination of the states <m>\ket{0}</m> and <m>\ket{1}</m>.</p> 
</section>

  <section xml:id="Vectors-and-Vector-Spaces">
    <title> Vectors and Vector Spaces</title>
    <subsection xml:id="Vectors">
      <title>Introduction to Vectors</title>
    <p>
      A vector is an ordered list of numbers that is used to describe quantities with both magnitude and direction. One example of a vector is force, which has both a magnitude (how strong the force is) and a direction (the angle at which the force is being applied). Each vector has a dimension, which is the number of components that comprise it. It is customary to signify that something is a vector by either drawing an arrow on top of it or bolding it. If we have a vector called <m>x</m>, we would write <m>\vec{x}</m> or <m>\boldsymbol{x}</m>. The number <m>x_{n}</m> is called the <m>n</m>-th component of <m>\vec{x}</m>. </p> <p>The most typical use of a vector with <m>n</m> components is to describe a point in <m>n</m> dimensional space in reference to some starting point. For example, if you have a square piece of paper and label the bottom left corner with the starting point (0,0), then a vector with components (1,1) would represent moving one unit of measurement along the bottom of the paper and one unit of measurement along the side of the paper to reach a new point. This vector would have a direction of <m>45^{\circ}</m> and a magnitude of <m>\sqrt{2}</m> units (by the pythagorean theorem). 
      </p>
      <image source="VectorExample.png">
        <shortdescription>An example of a vector</shortdescription>
      </image>
    </subsection>
    <subsection xml:id="Vector-Spaces">
      <title>Vector Spaces</title>
  <p> Every vector exists within a <term>vector space</term><idx><h>Vector</h><h>Vector Space</h></idx>, which are sets that satisfy certain mathematical properties. The most common vector spaces we will deal with are <m>\mathbb{R}^n</m>, the set of all <m>n</m> dimensional vectors with real components, and <m>\mathbb{C}^n</m>, the set of all <m>n</m> dimensional vectors with complex components. Notice <m>\mathbb{R}^n \subseteq \mathbb{C}^n</m>, so we will usually work with <m>\mathbb{C}^n</m> for generality. All vector spaces have the same properties regardless of dimension. Here are the properties for an <m>n</m>-dimensional vector space:   
    </p>
    <p>1. Vector equality: <m>\vec{x}=\vec{y}</m> means <m>x_{i}=y_{i}, i \in n. </m> </p>
      <p>2. Vector addition: <m>\vec{x}+\vec{y}=\vec{z}</m> means <m>x_{i}+y_{i}=z_{i}, i \in n. </m> </p>
        <p>3. Scalar multiplication: <m>a \vec{x} \equiv \big(a x_{1}, a x_{2},..., a x_{n}\big). </m> </p>
          <p>4. Negative of a vector: <m>-\vec{x}=(-1) \vec{x} \equiv \big(-x_{1},-x_{2},...,-x_{n}\big).  </m> </p>
            <p>5. Null vector: There exists a null vector <m>\vec{0} \equiv (0,0,...,0).  </m> </p>
<p> If our vector components are all real numbers (i.e. the vector space exists in <m>\mathbb{R}^n</m> instead of <m>\mathbb{C}^n</m>), then the following properties also hold: </p>
<p>1. Addition of vectors is commutative: <m>\vec{x}+\vec{y}=\vec{y}+\vec{x}. </m></p>
<p>2. Addition of vectors is associative: <m>(\vec{x}+\vec{y})+\vec{z}=\vec{x}+(\vec{y}+\vec{z}). </m></p>
<p>3. Scalar multiplication is distributive: <m>a(\vec{x}+\vec{y})=a \vec{x}+a \vec{y},</m> and <m>(a+b) \vec{x}=a \vec{x}+b \vec{x}</m></p>
<p>4. Scalar multiplication is associative: <m>(a b) \vec{x}=a(b \vec{x}).</m> </p>
</subsection>
<subsection xml:id="Bases">
  <title>Bases</title>
  <p> For any vector space, one can find a subset of vectors which can be used to generate any other vector in the space through linear combinations (scalar multiplication and vector addition). The smallest set of vectors that fulfills this property is called the <term>basis</term><idx><h>Vector</h><h>basis</h></idx>. In <m>\mathbb{R}^{2}</m>, we only need two vectors to produce the rest through linear combination. The standard basis, <m>\mathcal{B},</m> is:  </p>
  <me>
    {\mathcal{B}}=\bigg\{ \{\widehat{\mathbf{x}}, \widehat{\mathbf{y}}\}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \bigg\}
  </me>
  <p> The ^ symbol is used to denote that a vector is <term>normal</term><idx><h>Vector</h><h>Normal</h></idx>, which means that it has a length of 1. The vector <m>\widehat{x}</m> is referred to as "<m>x</m> hat." This property is extremely important to quantum mechanics and will be discussed more later. </p>

<p> Bases have two properties: </p>
<p> <alert>Linear Independence: </alert> A set of vectors is <term>linearly independent</term><idx><h>Vector</h><h>Linearly Independent</h></idx> if we cannot express any one of them as a linear combination of the others. If we can express one as a linear combination of the others, then it is called a <term>linearly dependent</term><idx><h>Vector</h><h>Linearly Dependent</h></idx> set. A basis must be linearly independent. </p>
<p> <alert>Completeness: </alert> A set of vectors is complete if it <term>spans</term><idx><h>Vector</h><h>Span</h></idx> its vector space, which in turn means that any vector in the space can be expressed as a linear combination of the vectors in that set. A basis for a vector space must span the space. </p>
</subsection>
<example>
  <title>(A Counterexample)</title>
<p>Let <m>{\mathcal{B}}</m> be the set,</p>
<me> {\mathcal{B}} = \Bigg\{ \begin{pmatrix} 1\\ 0\\ 0\\ \end{pmatrix}, \begin{pmatrix} 0\\ 1\\ 0\\ \end{pmatrix} \Bigg\} </me>
  <p> And let <m>\vec{v}</m> be the vector, </p>
  <me> \vec{v}= \begin{pmatrix} 2\\ -3\\ 1\\ \end{pmatrix} </me>
  <p> Since we are unable to express <m>\vec{v}</m> as a linear combination of the elements of <m>\mathcal{B}</m>, then we say <m>{\mathcal{B}}</m> is not complete. </p>
</example>
<example>
  <p>Let <m>{\mathcal{B}}</m> be the set,</p>
  <me> {\mathcal{B}} = \Bigg\{ \begin{pmatrix} 1\\ 0\\  \end{pmatrix}, \begin{pmatrix} 0\\ 1\\ \end{pmatrix} \Bigg\} </me>
  <p> And let <m>\vec{x}</m> be the vector, </p>
  <me> \vec{x} = \begin{pmatrix} 5\\ -7\\ \end{pmatrix} </me>
  We can express <m>\vec{x}</m> as:
  <me> \vec{x} = 5 \cdot \begin{pmatrix} 1\\ 0\\ \end{pmatrix} + (-7) \cdot \begin{pmatrix} 0\\ 1\\ \end{pmatrix} = \begin{pmatrix}5\\0\\ \end{pmatrix} + \begin{pmatrix}0\\ -7\\ \end{pmatrix} = \begin{pmatrix} 5\\ -7\\ \end{pmatrix} </me>
  <p> Since we can express <m>\vec{x}</m> as a linear combination of the elements of <m>\mathcal{B}</m> and it is easy to show that we could construct any other vector in <m>\mathbb{R}^2</m> from these same elements, we say that <m>\mathcal{B}</m> spans <m>\mathbb{R}^2</m> and is complete. </p>
</example>
<theorem xml:id="thm-1"><statement><p><alert> Dimension of a basis. </alert> The number of basis elements for a vector space is the same as that spaces dimension.  </p></statement></theorem>

<subsection xml:id="linear-algebra">
  <title>Linear Algebra</title>
  <p>Linear algebra is the study of vectors and transformations. In this subsection we will describe some other pieces of linear algebra that will be important to quantum computation.</p>
 <p><alert>Vector Transpose:</alert> The <term>transpose</term><idx><h>Vector</h><h>Transpose</h></idx> is an operation that turns a standard column vector into a row vector, or vice versa. This means an <m>n</m> dimensional vector changes from having <m>n</m> rows and <m>1</m> column to having <m>1</m> row and <m>n</m> columns. The transpose is represented with a superscript <m>T</m> and the operation is shown below. </p>
<me> \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}^{T} = \begin{pmatrix} a_1 &amp; a_2 &amp; \ldots &amp; a_n \end{pmatrix} </me>
<p><alert>Dot Product / Inner Product:</alert> The dot product (more generally known as the <term>inner product</term><idx><h>Vector</h><h>Inner Product</h></idx> in the context of quantum computation) is an operation between two vectors of the same dimension that produces a scalar. This product is typically referred to with a <m>\cdot</m>, but has an alternate notation in quantum computation that we will see in the next section. In <m>\mathbb{R}^n</m> and In <m>\mathbb{C}^n</m> this operation is performed by taking the sum of the products of the corresponding entries in each vector, as shown below.  </p>
<me> \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix} \cdot \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{pmatrix} = a_1 \cdot b_1 + a_2 \cdot b_2 + \ldots + a_n \cdot b_n </me>
<example>
  <me> \begin{pmatrix} 5 \\ 3 \\ 7\\ 2 \end{pmatrix} \cdot \begin{pmatrix} 6 \\ 0 \\ 5 \\ 1 \end{pmatrix} = 5 \cdot 6 + 3 \cdot 0 + 7 \cdot 5 + 2 \cdot 1 = 67 </me>
</example>
<exercise>
  <statement><p> Find the inner product <m>\vec{v_1} \cdot \vec{v_2} </m> </p>
    <me>\vec{v_1} = \begin{pmatrix} 8 \\ -4 \\ 5 \end{pmatrix}, \text{  } \vec{v_2} = \begin{pmatrix} 3 \\ 5 \\ -2 \end{pmatrix} </me> </statement> 
    <solution><me> \begin{pmatrix} 8 \\ -4 \\ 5 \end{pmatrix} \cdot \begin{pmatrix} 3 \\ 5 \\ -2 \end{pmatrix} = 8 \cdot 3 + -4 \cdot 5 + 5\cdot -2 = -6 </me></solution>
</exercise>
<p><alert>Orthogonality:</alert> <term>Orthogonality</term><idx><h>Orthogonality</h></idx> is the generalization of the concept of perpendicularity. In two and three dimensional space, two vectors are orthogonal if the angle between them is a right angle. Two vectors are orthogonal if their inner product is equal to <m>0</m>.</p>
<exercise>
  <statement><p> Are the following vectors orthogonal? </p>
  <me>\begin{pmatrix} 4 \\ 3 \\ -2 \end{pmatrix}, \begin{pmatrix} 1 \\ 2 \\ 5 \end{pmatrix}</me></statement>
  <solution><p>Yes,</p><me> \begin{pmatrix} 4 \\ 3 \\ -2 \end{pmatrix} \cdot \begin{pmatrix} 1 \\ 2 \\ 5 \end{pmatrix} = 4 \cdot 1 + 3 \cdot 2 + -2 \cdot 5 = 0</me><p>Since the inner product between the two vectors is 0, they are orthogonal</p></solution> 
</exercise>
<p><alert>Normality:</alert> A vector is <term>normal</term><idx><h>Vector</h><h>Normal Vector</h></idx> if it has a length of <m>1</m>. The length (sometimes also referred to as the norm) of a vector can be found be taking the square root of the sum of the squares of its entries, as shown below. A non-normal vector can be normalized by dividing each of its components by the vectors length. A normalized vector has the same direction as the original vector, but has a length of <m>1</m>. A set of vectors is <term>orthonormal</term><idx><h>Vector</h><h>Orthonormal Vectors</h></idx> if each of the vectors are normal and each of the vectors are orthogonal to the rest. </p>
<me> \text{Length} (\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}) = \sqrt{a_{1}^2 + a_{2}^2 + \ldots + a_{n}^2} </me>
<exercise>
  <statement><p>What is the length of the following vector?</p><me>\begin{pmatrix} 7 \\ 2 \\ 3 \end{pmatrix}</me></statement> 
  <solution> <me> \sqrt{7^2 + 2^2 + 3^2} = \sqrt{62} \approx 7.874 </me> </solution> 
</exercise>  
<exercise>
  <statement><p>Normalize the following vector</p><me>\begin{pmatrix} 7 \\ 2 \\ 3 \end{pmatrix}</me></statement>
  <solution><p>In the previous exercise, we found that this vector has a length of <m>\sqrt{62}\approx7.874</m>. To normalize the vector, we will divide each of its components by its length.</p>
    <me> \begin{pmatrix} \frac{7}{\sqrt{62}} \\ \frac{2}{\sqrt{62}} \\ \frac{3}{\sqrt{62}} \end{pmatrix} \approx \begin{pmatrix} 0.889 \\ 0.254 \\ 0.381 \end{pmatrix}</me></solution>
</exercise>
<p><alert>Matrices:</alert> Whereas a vector is a single column of elements, a <term>matrix</term><idx><h>Matrix</h></idx> is a table of elements organized in rows and columns. Technically speaking, a vector can be thought of a matrix with only one column. The dimension of a matrix is described by first listing the number of rows and then listing the number of columns. Thus, a <m>2 \times 3</m> matrix (read "two by three") would have two rows and three columns. A <term>square matrix</term><idx><h>Matrix</h><h>Square Matrix</h></idx> is any matrix with the same number of rows and columns. One of the most important matrices is the <term>identity matrix</term><idx><h>Matrix</h><h>Identity Matrix</h></idx>, a square matrixin which all of the entries along the diagonal are <m>1</m> and all other entries are <m>0</m>. Examples of the <m>2\times2</m> and <m>5\times5</m> identity matrices are shown below, which can be generalized to any <m>n \times n</m> matrix.</p>
  <me> I_{2\times2} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, I_{5\times5} = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1   \end{pmatrix}</me>
  <p> A matrix can be multiplied by a scalar in the same way that a vector can, by multiplying each entry in the matrix by the scalar, as shown below. </p>
    <me> c \times \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} = \begin{pmatrix} c \times a_1 &amp; c \times a_2 &amp; c \times a_3 \\ c \times a_4 &amp; c \times a_5 &amp; c \times a_6 \\ c \times a_7 &amp; c \times a_8 &amp; c \times a_9 \end{pmatrix} </me>
 <p> The concept of a transpose can be extended from vectors to matrices as well. A matrix's transpose is found by turning each of its rows into a column, or, equivalently, by turning each of its columns into a row. This means the transpose of an <m>m \times n</m> matrix is an <m>n \times m</m> matrix. The matrix transpose is also represented by a superscript <m>T</m>. An example of transposing a <m>3 \times 3</m> matrix is shown below. </p>
 <me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} ^{T} = \begin{pmatrix} a_1 &amp; a_4 &amp; a_7 \\ a_2 &amp; a_5 &amp; a_8 \\ a_3 &amp; a_6 &amp; a_9 \end{pmatrix} </me>
    <p><alert>Matrix Addition:</alert> Two matrices can be added together only if they each have the same number of rows and columns. The sum of two matrices is found by adding together the corresponding entries of each matrix, as shown below with an example of two <m>3\times3</m> matrices. </p>
  <me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} + \begin{pmatrix} b_1 &amp; b_2 &amp; b_3 \\ b_4 &amp; b_5 &amp; b_6 \\ b_7 &amp; b_8 &amp; b_9 \end{pmatrix} = \begin{pmatrix} a_1 + b_1 &amp; a_2 + b_2 &amp; a_3 + b_3 \\ a_4 + b_4 &amp; a_5 + b_5 &amp; a_6 + b_6 \\ a_7 + b_7 &amp; a_8 + b_8 &amp; a_9 + b_9 \end{pmatrix} </me>
  <exercise> 
  <statement>
    <p> Compute the following sum: </p>
    <me> \begin{pmatrix} 4 &amp; 7 &amp; 9 \\ 1 &amp; 0 &amp; 6 \\ 2 &amp; 5 &amp; 3 \end{pmatrix} + \begin{pmatrix} 0 &amp; 4 &amp; 6 \\ 2 &amp; 2 &amp; 2 \\ 5 &amp; 3 &amp; 8 \end{pmatrix}</me>
  </statement>  
  <solution>
    <me> = \begin{pmatrix} 4 &amp; 11 &amp; 15 \\ 3 &amp; 2 &amp; 8 \\ 7 &amp; 8 &amp; 11 \end{pmatrix} </me>
  </solution>
  </exercise>
<p><alert>Matrix Multiplication:</alert> Two matrices can be multiplied together only if the number of columns of the left matrix is equal to the number of rows of the right matrix. The resulting product will be a matrix with the same number of rows as the left matrix and the same number of columns as the right. Thus an <m>m /times n</m> matrix multiplied by a <m> n \times q</m> matrix would produce an <m>m \times q</m> matrix. Notably, matrix multiplication is non-commutative, which means for two matrices <m>A</m> and <m>B</m>, <m>AB \neq BA</m>. The entries of a product matrix are determined by taking the dot product between the corresponding row of the left matrix and the corresponding column of the right matrix, as shown below with an example of multiplication between a <m>3\times3</m> and a <m>3\times2</m> matrix. </p>
<me> \begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\ a_4 &amp; a_5 &amp; a_6 \\ a_7 &amp; a_8 &amp; a_9 \end{pmatrix} \times \begin{pmatrix} b_1 &amp; b_2 \\ b_3 &amp; b_4 \\ b_5 &amp; b_6 \end{pmatrix} = \begin{pmatrix} a_1b_1 + a_2b_3+a_3b_5 &amp; a_1b_2 + a_2b_4 + a_3b_6 \\ a_4b_1 + a_5b_3 + a_6b_5 &amp; a_4b_2 + a_5b_4 + a_6b_6 \\ a_7b_1 + a_8b_3 + a_9b_5 &amp; a_7b_2 + a_8b_4 + a_9b_6 \end{pmatrix} </me>
<p> Now that we know matrix multiplication, we can redefine the dot product / inner product as the transpose of a vector multiplied by the original vector. Since vectors are <m>n \times 1</m> dimensional, the transpose will be <m>1 \times n</m> dimensional, so multiplying the transpose on the left and the original on the right will produce a <m>1 \times 1</m> matrix, which is functionally the same as a scalar. </p>
<exercise>
  <statement> <p> Compute the following product: </p> <me> \begin{pmatrix} 2 &amp; 4 \\ 3 &amp; 0 \end{pmatrix} \times \begin{pmatrix} 5 &amp; 1 &amp; 2 \\ 0 &amp; 7 &amp; 2 \end{pmatrix} </me> </statement>
    <hint> <p> The resulting matrix should be <m>2 \times 3</m> </p></hint>
    <solution> <me> = \begin{pmatrix} 2 \times 5 + 4 \times 0 &amp; 2 \times 1 + 4 \times 7 &amp; 2 \times 2 + 4 \times 2 \\ 3 \times 5 + 0 \times 0 &amp; 3 \times 1 + 0 \times 7 &amp; 3 \times 2 + 0 \times 2 \end{pmatrix} = \begin{pmatrix} 10 &amp; 30 &amp; 12 \\ 15 &amp; 3 &amp; 6 \end{pmatrix} </me> </solution> </exercise>
<p><alert>Determinant:</alert> The <term>determinant</term><idx><h>Determinant</h></idx> is a scalar associated with a square matrix that can be used to find the eigenvalues (see <xref ref="subsec-Eigenvectors-and-Eigenvalues"/>) of the matrix. For <m>2 \times 2</m> and <m>3 \times 3</m> matrices, their determinants are defined as follows: </p>
<me> det(\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}) = a \cdot d - b \cdot c </me>
<me> det(\begin{pmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{pmatrix}) = a \cdot e \cdot i +  b \cdot f \cdot g + c \cdot d \cdot h - c \cdot e \cdot g - b \cdot d \cdot i - a \cdot f \cdot h </me>
  </subsection>
<subsection xml:id="subsec-Eigenvectors-and-Eigenvalues">
  <title>Eigenvectors and Eigenvalues</title>
  
</subsection>
</section>

<section xml:id="Linear-Algebra-in-Quantum-Computation">
  <title>Linear Algebra in Quantum Computation</title>
 <p> As mentioned in the section on Classical Computing <xref ref="Classical-Computing"/>, qubits can be in the state <m>\ket{0}</m> or <m>\ket{1}</m>. What's more, these states can be represented with vectors. </p>
  <me> \ket{0} = \begin{pmatrix} 1 \\ 0 \end{pmatrix} </me>
  <p> and </p>
  <me> \ket{1} = \begin{pmatrix} 0 \\ 1 \end{pmatrix} </me>
  <p> This notation was first introduced by mathematician Paul Dirac and is known as "Dirac Notation" or "<term>Bra-Ket Notation</term><idx><h>Bra-Ket Notation</h></idx>." In this notation, the <m>\ket{}</m> symbol represents a qubit state and is referred to as a "<term>ket</term><idx><h>Bra-Ket Notation</h><h>Ket</h></idx>." At a glance, it can be seen that the vectors <m>\ket{0}</m> and <m>\ket{1}</m> are each normal and are orthogonal to each other. Additionally, any point in two dimensional space could be described with a linear combination of these two vectors, meaning they form a basis (we will discuss exactly which space they form a basis for in the following section). Put together, this means <m>\ket{0}</m> and <m>\ket{1}</m> form an orthonormal basis. All qubits must be normalized in order to be expressed properly. </p>
  <p> Systems with multiple qubits are described by vectors in higher dimensions, which will be discussed later. </p>
</section>

<section xml:id="Qubit-States">
  <title>Qubit States</title>
  <subsection xml:id="subsec-Qubit">
    <title>Qubits</title>
 <p> As a reminder, a qubit exists in a superposition between the states <m>\ket{0}</m> and <m>\ket{1}</m>, which means that they can be expressed as a linear combination of the vectors <m>\begin{pmatrix} 1 \\ 0 \end{pmatrix}</m> and <m> \begin{pmatrix} 0 \\ 1 \end{pmatrix} </m>. This means a qubit <m> \psi </m> can be expressed </p>
 <me> \ket{\psi} = \alpha \ket{0} + \beta \ket{1} </me>
 <p> Where <m> \alpha </m> and <m> \beta </m> are complex coefficiants that relate to the probability that a qubit is in the states <m> \ket{0}</m> and <m> \ket{1} </m> respectively. This means that qubits exist within a complex vector space. The exact space that qubits exist within is known as a <term>hilbert Space</term><idx><h>Hilbert Space</h></idx>. </p>
 </subsection>
<subsection xml:id="subsec-Hilbert-Space">
  <title>Hilbert Space</title>
  <p> A hilbert space is a complex vector space in which the inner product is defined as an operation. A single qubit exists within the hilbert space <m>\mathbb{C}^2</m>, which is the set of all two dimensional vectors with complex entries. </p>
 </subsection>
 <subsection xml:id="subsec-Bra-Vectors-and-the-Inner-Product">
  <title>Bra Vectors and the Inner Product for Quantum Computation</title>
  <p> We now introduce the bra in Bra-Ket Notation. Each vector representation of a qubit has a corresponding <term>bra</term><idx><h>Bra-Ket Notation</h><h>Bra</h></idx> vector (sometimes also referred to as the "Dual Vector") that is equal to the complex conjugate of the transpose of the ket. This operation of taking the transpose and complex conjugating is known as the <term>adjoint</term><idx><h>Adjoint</h></idx> and is represented by the <m>\dagger</m> symbol. Bra vectors are represented by the symbol <m>\bra{}</m>. An example of finding the bra vector for the ket <m>\psi</m> is shown below (where the <m>*</m> symbol represents complex conjugating).</p>
<me> \ket{\psi}^{\dagger} = (\alpha \ket{0} + \beta \ket{1})^{\dagger} = (\alpha \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \beta \begin{pmatrix} 0 \\ 1 \end{pmatrix})^{\dagger} = \alpha^{*} \begin{pmatrix} 1 &amp; 0 \end{pmatrix} + \beta^{*} \begin{pmatrix} 0 &amp; 1 \end{pmatrix} = \bra{\psi} </me>
<p> Bra vectors allow us to use a new notation for the inner product. We write the inner product between two vectors, <m>\ket{\psi}</m> and <m>\ket{\phi}</m>, as <m>\braket{\psi | \phi}</m> and perform the calculation as matrix multiplication, as described in <xref ref="linear-algebra"/> </p>
<exercise>
  <statement><p>Find the inner product between the following two qubits.</p>
  <me> \ket{\psi} = \frac{1}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1} = \begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix} </me>
  <me> \ket{\phi} = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt2} \ket{1} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} </me></statement>
<solution>
<me> \ket{\psi} \cdot \ket{\phi} = \braket{\psi|\phi} = \begin{pmatrix} \frac{1}{2} &amp; \frac{\sqrt{3}}{2} \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} = \frac{1}{2} \cdot \frac{1}{\sqrt{2}} + \frac{\sqrt{3}}{2} \cdot \frac{1}{\sqrt{2}} = \frac{1 + \sqrt3}{2\sqrt{2}}</me></solution>
</exercise>
</subsection>
 <subsection xml:id="subsec-Qubit-Measurements">
  <title>Qubit Measurements</title>
  <p> When a qubit is measured, it collapses from being a superposition of the states <m>\ket{0}</m> and <m>\ket{1}</m> to being in one state or the other. The probability that the qubit will be in either state is related to the coefficient on that state. For a qubit <m> \psi </m>, </p>
<me> \ket{\psi} = \alpha \ket{0} + \beta \ket{1} = \alpha \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \beta \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} \alpha \\ \beta \end{pmatrix} </me> 
<p>Since all qubits are normalized, <m>\psi</m> has a length of <m>1</m>, so <m>\sqrt{\alpha^2 + \beta^2}=1</m>, which means <m>\alpha^2 + \beta^2=1</m>.  Since the combined probability that the qubit will be in the state <m>\ket{0}</m> or <m>\ket{1}</m> is equal to <m>1</m>, we use the normal property of qubits as a method to determine the probability that the qubit will be in either state. For the qubit <m>\psi</m> the probability that it will be in the state <m>\ket{0}</m> after it is measured is <m>\alpha^2</m> and the probability that it will be in the state <m>\ket{1}</m> is <m>\beta^2</m>. </p>
<exercise>
  <statement><p>For the following qubit, find the probability that it will be in the state <m>\ket{0}</m> and the probability that it will be in the state <m>\ket{1}</m> after measurement</p>
  <me> \ket{\psi} = \frac{1}{2} \ket{0} + \frac{\sqrt{3}}{2} \ket{1} </me></statement>
<solution><p> The probability that <m>\ket{\psi}</m> will be in the state <m>\ket{0}</m> after measurement is 25% because <m>(\frac{1}{2})^2 = 1/4 = 0.25</m>. The probability that <m>\ket{\psi}</m> will be in the state <m>\ket{1}</m> after measurement is 75% because <m>(\frac{\sqrt{3}}{2})^2 = 3/4 = 0.75</m>. </p></solution>
</exercise>
<p> Another way to find the probability that a qubit is in any given state is to take the square of the inner product of the outcome state and the qubit. For a qubit <m>\ket{\psi}</m>, the probability that it is in the state <m>\ket{0}</m> after measurement would be <m>(\braket{0|\psi})^2</m> and the probability that it is in the state <m>\ket{1}</m> would be <m>(\braket{1|\psi})^2</m>.</p>
<exercise>
  <statement><p> Using an inner product, calculate the probability that the following system is in the state <m>\ket{0}</m> and <m>\ket{1}</m>.</p>
  <me> \ket{\psi} = \frac{\sqrt{7}}{\sqrt{8}} \ket{0} + \frac{1}{\sqrt{8}} \ket{1} = \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix} </me></statement>
<solution> <me> P(0) = (\braket{0|\psi})^2 = (\begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix}+0)^2 = (\frac{\sqrt{7}}{\sqrt{8}})^2 = 7/8 = 0.875 </me> <p> The probability that <m>\ket{\psi}</m> will collapse into the state <m>\ket{0}</m> is 87.5%. </p> 
<me> P(1) = (\braket{1|\psi})^2 = (\begin{pmatrix} 0 &amp; 1 \end{pmatrix} \begin{pmatrix} \frac{\sqrt{7}}{\sqrt{8}} \\ \frac{1}{\sqrt{8}} \end{pmatrix})^2 = (0+\frac{1}{\sqrt{8}})^2 = 1/8 = 0.125 </me> <p> The probability that <m>\ket{\psi}</m> will collapse into the state <m>\ket{1}</m> is 12.5%. </p> </solution></exercise>
</subsection>
</section>

<section xml:id="Computational-Bases">
  <title>Computational Bases and the Block Sphere</title>
  <p> So far, the only basis we have looked at for measuring a qubit is <m>(\ket{0},\ket{1})</m>, but this is not the only one. </p>





  <figure xml:id="fig-Bloch-Sphere">
    <caption>The Bloch Sphere and the representation of a <m>\ket{\psi}</m>
      state vector with angle <m>\phi</m> along the horizontal plane and angle <m>\theta</m> along the vertical plane, as well as the computational basis vectors. </caption>
  <image source="BlochSphere.png" width="60%"> </image> 
  </figure>
</section>

<section xml:id="Linear-Operators">
  <title>Linear Operators</title>
  
</section>

<section xml:id="sec-Systems-of-Multiple-Qubits">
  <title>Systems of Multiple Qubits</title>
  
</section>

</chapter>